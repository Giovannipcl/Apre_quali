---
title: "Bayesian Conditional Transformation Models using Laplace approximation"
author: 
  - Giovanni Pastori Piccirilli
  - Márcia D'Elia Branco
date: "21 September 2023"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney2.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling
---

```{r, load_refs, echo=FALSE, cache=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'authoryear', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
myBib <- ReadBib("assets/bibliografia.bib", check = FALSE)
top_icon = function(x) {
  icons::icon_style(
    icons::fontawesome(x),
    position = "fixed", top = 10, right = 10
  )
}


```

<style>
p.caption {
  font-size: 0.6em;
}
</style>

### Overview
___

- Introduction

- Conditional Transformation Models

- Bayesian Conditional Transformation Models
  - B-spline and Bernstein basis
  - Prior specification

- Properties of the Bayesian Conditional Transformation Models

- Posterior inference
  - Integrated Laplace with Bayesian Conditional
Transformation Models
  - Variational Bayes correction to BCTM
  - Posterior predictive check



---

- Simulation studies
  - Study 1: Evaluating the ILBCTM performance
  - Study 2: BCTM with Bernstein polynomials

- Applications 
  - Framingham heart study
  - Sleep deprivation study

- Original dataset applications
  - Modeling the mortality rate from bronchial and
lung cancer in Brazil
  - Analysis of vehicle theft incident in the city of São
Paulo

- Final Remarks

---

###Introduction 

___

- The primary objective of regression models is to estimate the conditional probability distribution $F_{Y|X=x}$ of a random variable $Y$, given an explanatory variable $X$ with an observed value $x$.
- Many regression models focus exclusively on the conditional expected value $E(Y|X=x)$, assuming that other moments of the distribution remain fixed or are only indirectly influenced by the explanatory variables.
- As a result, model inference often relies on strong assumptions, such as homoscedasticity and symmetry.
- An approach that guarantees the flexibilization of some assumptions in the class of regression models is the Generalized additive model for the location, scale, and shape (GAMLSS) class.
- The class includes a broad family of distributions for continuous and discrete responses incorporating highly skew or kurtotic distributions, however, choosing a parametric distribution on a large family of distributions has to be done carefully since it imposes assumptions on the model.
---

- To move beyond the parametric framework, quantile regression models `r Citep(myBib,"koenker2001quantile")` provide a flexible alternative to regression models. In quantile regression, each conditional quantile is modeled independently as a function of linear or smooth terms of the explanatory variables.
- Conditional Transformation Models (CTM) `r Citep(myBib,"hothorn2014conditional")` address the direct estimation of the conditional distribution function of a random variable $Y$ conditional on a set of covariates $X = x$.
-  The approach involves applying a transformation function $h(Y|x)$ to the random variable $Y$, where $x$ is a fixed and known value, in conjunction with a baseline distribution function.
- `r Citet(myBib,"carlan2023bayesian")` presented the Bayesian Conditional Transformation Model (BCTM) Building on the same definition as the CTM, this approach proposed a different parameterization for the transformation function $h(y|\boldsymbol{X})$ and utilized a Markov Chain Monte Carlo (MCMC) simulation algorithm to generate inferences about the parameters.
- We present an alternative estimation procedure based on the Laplace approximation, motivated by the Integrated Nested Laplace Approximation (INLA) approach `r Citep(myBib,"rue2009approximate")`.
- We tested our proposed algorithm to some models already fitted with other approaches (MCMC and Maximum likelihood).

---
- In this work, we aim to contribute to the CTM class and, more specifically, to the BCTM. We present two alternative and efficient Bayesian estimation procedures for the BCTM.
- The first is based on the Laplace approximation, inspired by the Integrated Nested Laplace Approximation (INLA). 

- However, applying these proposed algorithms requires some restrictions on the construction of the transformation function$h(y|\boldsymbol{X}) $. Despite this, these restrictions are minimal and do not affect the model's applicability in various scenarios.
- we also propose an alternative parameterization for the transformation functions $h(y|x) $, distinct from the one presented in \citet{carlan2023bayesian}. In this proposal, we utilize Bernstein bases, which allow us to express nonlinear functions as linear combinations of these bases.
- of this work is related to the interpretability of the effects of the explanatory variables $x$ on the distribution of the response variable $y $. The BCTM class of models suffers from a lack of interpretability due to its construction. In this work, we propose two different results that enable us to evaluate how a particular variable $x$ impacts both the expected value of $Y$ and the quantiles of the distribution of $Y $.



---
### Conditional Transformation Models

___

- A ideia de transformacao de variaveis aleatorias já é muito popular na área de modelagem. A trnasformacao Box-Cox é um exemplo
$$
Y(\lambda) = 
\begin{cases}
\frac{Y^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\
\log(Y) & \text{if } \lambda = 0,
\end{cases}
$$
- These transformations help reframe the distribution of the response variable, making it easier to model and analyze.

- The Conditional Transformation Model (CTM) extends the transformation models class by allowing the transformation functions depending on a set of M explanatory variables $\bfs{X} \in \mathbb{R}^{M}$.
- These new transformation functions from CTM are univariate functions of the response variable, conditional on fixed values of explanatory variables $\bfs{X} = \boldsymbol{x}$.

The functions are denoted by $h(Y|\boldsymbol{x}): \mathbb{R} \rightarrow \mathbb{R}$

---

- These conditional transformation models aim to estimate the conditional distribution function of a response variable $Y$, given an observed explanatory vector $\bfs{X} = \bfs{x}$, denoted as $F_{Y|\bfs{X} = \bfs{x}}$.

- It is achieved by estimating the transformation function $h(Y| \bfs{x})$ rather than assuming a fixed form and selecting a baseline distribution without parameters to be estimated.

- In the CTMs context exist a conditional transformation function $h(Y \mid \mathbf{x})$ such that the transformed variable follows a known probability distribution (baseline distribution).

- The conditional distribution function of a random variable $Y $, given a set of observed explanatory variables $\mathbf{x} $, is defined as follows:

\begin{equation}\label{ctm}
    F_{Y|\bfs{X} = \bfs{x}}(y) = P(Y \leq y|\boldsymbol{X} = \boldsymbol{x})= P(Y \leq y|\boldsymbol{X} = \boldsymbol{x}) = P(h(Y|\boldsymbol{x}) \leq h(y|\boldsymbol{x})) = F_Z(h(y|\boldsymbol{x}))
\end{equation}

\noindent where $F_Z(\cdot)$ is a continuous baseline cumulative distribution function (cdf) of the Gaussian distribution.

- The conditional transformation function $h(Y \mid \boldsymbol{x})$ is defined on the real line $\mathbb{R}$ and is monotonically increasing in $y \in \mathbb{R} $, conditional on $\boldsymbol{X} = \boldsymbol{x} $.

---

$$
    F_{Y|\bfs{X} = \bfs{x}}(y) = P(Y \leq y|\boldsymbol{X} = \boldsymbol{x})= P(Y \leq y|\boldsymbol{X} = \boldsymbol{x}) = P(h(Y|\boldsymbol{x}) \leq h(y|\boldsymbol{x})) = F_Z(h(y|\boldsymbol{x}))
$$

- The second equation holds because $h(Y \mid X = x)$ is monotonic in $y $. This ensures that $Y \leq y$ if and only if $h(Y) \leq h(y) $.

\textit{For all random variables $Y$ conditional on $\bfs{X} = \bfs{x}$, and for any random variable $Z $, there exists a unique strictly monotonically increasing transformation $g_x$ such that $F_{Y|\bfs{X} = \bfs{x}} = F_{g_x \circ Z}$ for a given value $\bfs{x}$ of $\bfs{X}$.}

For $Y$ conditional on $\bfs{X} = \bfs{x}$ a continuous random variable, there exists $g_x = h^{-1}(\cdot|\bfs{x}) $, such that $h(\cdot|\bfs{x}) = F_Z^{-1} \circ F_{Y|\bfs{X} = \bfs{x} }$ is strictly monotonic and right-continuous, with a well-defined first derivative.

---
- The task of obtaining $F_{Y|\bfs{X} = \bfs{x}}$ reduces to estimating the transformation function $h $.

- The construction of $h(y|\bfs{x})$ must account for all the characteristics and objectives of the proposed model.

- It is convenient to decompose the transformation function $h$ additively into $J$ partial transformation functions, defined for all $\bfs{x} \in \mathbb{R}^M $.


\begin{equation}\label{decomp_h}
    h(y|\boldsymbol{x}) = \sum_{j = 1}^J h_j(y|\boldsymbol{x}).
\end{equation}

- The transformation functions $h_j(y|\bfs{x})$ from (\ref{decomp_h}) can range from low-parameterized forms to complex structures with many parameters.

-  we have assumed that the transformation function $h(y \mid \bfs{x})$ is strictly monotonic, right-continuous, differentiable, and additively decomposed as in (\ref{decomp_h}).

- we parameterize the transformation function, making the definition of conditional transformation models more general. This parameterization is achieved using basis functions. A basis, in this context, refers to a set of functions that can be used to express other functions as linear combinations.

---

### Transformation function of random variables
___

- Now, from the corollaries we write $F_Y$ distribution as a function of a transformation function $h$ that belongs to space of function $\mathcal{H}$ defined as $\mathcal{H} = \lbrace h: \Xi \rightarrow \mathbb{R} \hspace{0.1cm} | \hspace{0.1cm} h(y_1) < h(y_2) \hspace{0.1cm} ,\forall \hspace{0.1cm} y_1 < y_2 \in \Xi \rbrace$.
- For now, we write the transformation as a linear function of its basis-transformed argument $y$ using basis function.
- First, let us parameterize the transformation function as a linear function of evaluated basis of $y$,
$$
\bfs{a}: \mathbb{R} \rightarrow \mathbb{R}^v,
$$
such that

$$
h(y) = \bfs{a(y)}^T \gamma$, $\gamma \in \mathbb{R}^v
$$
- The choice of the basis has to attend to the monotonically increasing restriction, and the first derivative $a(y)'$ must be available. 
- For an evaluated basis and a vector of coefficients, we estimate the $F_Y$ distribution through the decomposition $F_Z \circ \boldsymbol{a}(y)^T \boldsymbol{\gamma}$.
- To abroad the class of transformation functions, $h$ may depend on explanatory variables $\boldsymbol{X} \in \mathbb{R}^m$. 
-Let
$$\mathbb{R} \rightarrow \mathbb{R}^m$$
be a basis for explanatory variables.
-  The conditional transformation function $h(Y|\bfs{x})$ is parameterized as  a linear function of joint evaluate basis of $Y$ and $\bfs{x}$.

 $$\bfs{c}: \mathbb{R}^v \times \mathbb{R}^m  \rightarrow \mathbb{R}^{d(v, m)}$$
is parameterized as a linear function of joint evaluate basis of $Y$ and $\boldsymbol{x}$ denoted as $\boldsymbol{c}(y,\boldsymbol{x}): \Xi \times \mathcal{X}  \rightarrow \mathbb{R}^{d(P, Q)}$.

such that $h(y|\bfs{x}) = c(y, \bfs{x})^T\bfs{\gamma}, \boldsymbol{\gamma} \in \mathbb{R}^{d(v,m)}$

- The choice of the basis $\bfs{c}$ has to attend to the monotonically increasing restriction on $y$

- For an evaluated basis and a vector of coefficients, we estimate the $F_{Y|\bfs{X} = \bfs{x}}$ distribution through the decomposition $F_Z \circ \bfs{c(y,\bfs{x})}^T \bfs{\gamma}$.
---

- The monotonically increasing restriction has to be attend only on $y$ direction.
- All characteristics and objectives of the constructed model should be considered in $h(y|\boldsymbol{x})$ construction.
- The BCTM is based on additive decomposition of the transformation function $h$ into $J$ partial transformation function for all $\boldsymbol{x} \in \mathcal{X}$. 

$$\begin{equation}\label{decomp_h}
    h(y|\boldsymbol{x}) = \sum_{j = 1}^J h_j(y|\boldsymbol{x}).
\end{equation}$$
- Monotonically of $h_j$ is sufficient for $h$ being monotone.
---

### Example of a simple CTM
 We assume $J = 3$ in (\ref{decomp_h}) and
    \begin{equation}
        h(y \mid x) = h_1(y) + h_2(y \mid x) + h_3(x) = \gamma_0 + \gamma_1 \cdot y + \gamma_2 \cdot x \cdot y + \gamma_3 \cdot x.
    \end{equation}
    
- The first transformation function $h_1(y) = \gamma_1 \cdot y$ is a linear function of y .
- The second transformation function $h_2(y|\boldsymbol{x}) = \gamma_2 \cdot x \cdot y$ involves a linear interaction between $y$ and $x$.
- The last transformation function $h_3(\boldsymbol{x}) = \gamma_4 \cdot x$ describes marginal effects of $x$.

$$\begin{aligned}
     E(\gamma_0 + \gamma_1 \cdot Y + \gamma_2\cdot x \cdot Y + \gamma_3 \cdot x|\boldsymbol{x}) = E(Z) \\
     E(Y|\boldsymbol{x})(\gamma_1 + \gamma_2\cdot x)  = - (\gamma_0  + \gamma_3 \cdot x)\\
     E(Y|\boldsymbol{x}) = \frac{ -(\gamma_0  + \gamma_3 \cdot x) }{\gamma_1  + \gamma_2\cdot x}.\\
     V(\gamma_0 + \gamma_1 \cdot Y + \gamma_2\cdot x \cdot Y + \gamma_3 \cdot x|\boldsymbol{x}) = 1 \\
     V(Y(\gamma_1 + \gamma_2\cdot x)|\boldsymbol{x}) = 1 \\
     V(Y|\boldsymbol{x}) = \frac{1}{  (\gamma_1 + \gamma_2\cdot x)^2}. \\
    \end{aligned}$$
---
- The joint basis for this model is given by $\bfs{c}(y|\bfs{x})^{\top} = (1,y, y \cdot x,x)$, with $\bfs{\gamma} = (\gamma_0, \gamma_1, \gamma_2, \gamma_3)$.

\begin{equation}\label{constrainA}
    \frac{\partial h(y|\bfs{x})}{\partial y} = \frac{\partial \bfs{c}(y|\bfs{x})^{\top}\bfs{\gamma}}{\partial y} \geq 0.
\end{equation}

We have that $\bfs{c}'(y|\bfs{x})^{\top} = \frac{\partial \bfs{c}(y|\bfs{x})^{\top}\bfs{\gamma}}{\partial y} = (1, x, 0,0)$, so the constrain in (\ref{constrainA}) is 

$$\bfs{c}'(y|\bfs{x})^T \bfs{\gamma}  = \gamma_1 + x \cdot \gamma2 \geq 0$$.



---


- Assuming linear transformation of the response variable, like $h(y) = y \cdot \gamma$ or even $h(y|x) = y \cdot x \cdot \gamma$, will only give us Gaussian models. 
- To leave the class of Gaussian models, we have to increase the $h(y)$ complexity. 
- This transformation can take a familiar form, such as $\log$. If we consider non-linear monotone functions the range of options is very large. 
- An alternative is to relax the specification of the transformation function and return for our basis specification.
- We must pick up a basis that defines a space of functions which our transformation function belongs.
- But many questions and challenges arise from this approach.
  - The functions should express complex functions, but they should preferably be smooth.
  - How much smooth should they be?
  - They should be monotone.

  
---

### Bayesian Conditional Transformation Models
___

- In the previous section, we introduced the definition of CTMs.

- Bayesian Conditional Transformation Models (BCTMs) extend CTMs by incorporating a Bayesian framework for inference.

- The first presentation of BCTMs was given by \citet{carlan2023bayesian}, where the author proposed the use of monotonic B-spline bases \citep{pya2015shape} to model conditional transformation functions. 

- Additionally, an MCMC algorithm based on the No-U-Turn Sampler (NUTS) \citep{hoffman2014no} was employed to sample from the posterior distributions of the model parameters.

- The construction of BCTMs follows the same principles as CTMs. However, the Bayesian framework naturally introduces penalization and smoothness in the B-spline bases through the choice of prior distributions on the basis coefficients. 

- Consequently, inferences about model parameters can be conducted using Highest Posterior Density (HPD) intervals and posterior probability calculations.

- The assessment of model fit can be performed using goodness-of-fit measures based on the posterior predictive distribution


---


### B-splines basis
___

**Definition:** Let k be a non-negative integer, $\boldsymbol{t}$ be the node vector of a non-decreasing sequence of real numbers of length at least k + 2, and $Q$ the length of $\boldsymbol{t}$. The jth B-spline of degree k with nodes $\boldsymbol{t}$ is defined by

$$B_{j,k,\boldsymbol{t}}(y) = \frac{y - t_j}{t_{j + k}- t_j} B_{j,k-1,\boldsymbol{t}}(y) + \frac{t_{j + 1 + k} - y}{t_{j + 1 + k}- t_{j + 1}} B_{j + 1,k - 1,\boldsymbol{t}}(y), j =1, \dots, Q,$$

$$B_{j,0,\boldsymbol{t}}(y)  = \left\lbrace\begin{array}{lc}
        1, & t_j \leq y \leq t_{j + 1}  \\
         0 &  \textrm{othwerwise}
    \end{array}\right.$$
 
- A B-spline form for some function $f$ requires: two integers, $k$ and $q$, defining respectively the degree and the number of linear parameters, a vector of knots $\boldsymbol{t}$ of length $2k + q$ in increasing order, where the $t_{k + 1}, \dots, t_q$ are the inner knots.
- The interval over which the spline is to be evaluated lies within $t_{k + 1}, \dots, t_q$. The first and last $k$ knot locations are arbitrary.
- On the other hand, we can define a basis by choosing only $k$ and the inner knots. In this case, $q = q' + k - 1.$ The length of vector knots is $q' + 3k - 1$.
---

- For degree $k = 1$, with a vector of inner knots $(2,3, \dots, 8)$ of length 7, we should have a vector of knots $\boldsymbol{t} = (t_1, 2,3, \dots, 8, t_9)$. The total of curve and linear parameters is $q = 7 + k - 1$.
- j = 1
$$\small{ B_{1,1,\boldsymbol{t}}(y) = \frac{y - 2}{3- 2} B_{1,0,\boldsymbol{t}}(y) + \frac{4 - y}{4 - 3} B_{2,0,\boldsymbol{t}}(y), j =1, \dots, 7, }$$

```{r,results='asis', echo=FALSE, message=FALSE, warning = FALSE,  fig.retina=2, fig.align='center',fig.width=4, fig.height=4}
#1
library("splines")
k = 1
knotsy = c(1,2,3,4,5,6,7,8,10)
x = seq(2,8,0.1)
Rn = cbind(as.vector(splineDesign(knotsy,x[1],ord = 2, outer.ok = T)), seq(1,7,1), rep(x[1],7 - 1 + k))
Rf = Rn

for(i in 2:length(x)){
  Rn = cbind(as.vector(splineDesign(knotsy,x[i],ord = 2, outer.ok = T)),seq(1,7,1), rep(x[i],7 - 1 + k))
  Rf = rbind(Rf,Rn)
}
Rf = as.data.frame(Rf)


library(ggplot2)

library("plotly")
library("crosstalk")
names(Rf) = c("B","V2","y")
tx = highlight_key(Rf, ~V2)
widgets <- bscols(filter_checkbox("V2", "j", tx, ~V2))
bscols(
  widths = c(2, 10), widgets,   plot_ly(tx,
                                        x = ~y, 
                                        y = ~B,
                                        color = ~factor(V2),
                                        type = 'scatter',
                                        mode = 'lines', 
                                        line = list(simplyfy = F)
  ) %>% 
    
    layout(title= list(text = "B-splines of degree 1"),
           xaxis = list(title = list(text ='y')), autosize = F, width = 700, height = 350)
)
```

---

- For degree $k = 2$ with a vector of inner knots $(2,3, \dots, 8)$ of length 7 we should have a vector of knots $\boldsymbol{t} = (t_1, t_2, 2,3, \dots, 8, t_{10}, t_{11}, t_{12})$.  The total of curve and linear parameters is $q = 7 + 2 -1 = 8$.
- j = 1
$$\small{  B_{1,2,\boldsymbol{t}}(y) = \frac{y - 2}{3- 2} B_{1,1,\boldsymbol{t}}(y) + \frac{4 - y}{4 - 3} B_{2,1,\boldsymbol{t}}(y), j =1, \dots, 8,}$$
```{r,results='asis', echo=FALSE, message=FALSE, warning = FALSE,  fig.retina=2, fig.align='center',fig.width=4, fig.height=4}
##### 2
k = 2
knotsy = c(1,1,2,3,4,5,6,7,8,10,10)
x = seq(2,8,0.1)
Rn = cbind(as.vector(splineDesign(knotsy,x[1],ord = 3, outer.ok = T)), seq(1,8,1), rep(x[1],7 - 1 + k))
Rf = Rn

for(i in 2:length(x)){
  Rn = cbind(as.vector(splineDesign(knotsy,x[i],ord = 3, outer.ok = T)),seq(1,8,1), rep(x[i],7 - 1 + k))
  Rf = rbind(Rf,Rn)
}
Rf = as.data.frame(Rf)

names(Rf) = c("B","V2","y")
tx = highlight_key(Rf, ~V2)
widgets <- bscols(filter_checkbox("V2", "j", tx, ~V2))
bscols(
  widths = c(2, 10), widgets,   plot_ly(tx,
                                        x = ~y, 
                                        y = ~B,
                                        color = ~factor(V2),
                                        type = 'scatter',
                                        mode = 'lines', 
                                        line = list(simplyfy = F)
  ) %>% 
    
    layout(title= list(text = "B-splines of degree 2"),
           xaxis = list(title = list(text ='y')), autosize = F, width = 650, height = 350)
)
```

---

- We will represent our transformation function as a linear combination of B-spline basis of degree $k = 3$ and a vector of parameters $\boldsymbol{\gamma}$

$$h_j(y) = \sum_{i = 1}^q B_{i,3,\boldsymbol{t}}(y) \gamma_{j,i} =  \boldsymbol{a}(y)^{\top} \boldsymbol{\gamma}_j,$$
with $\bfs{a}(y) = (B_{1,3,\bfs{t}}(y), B_{2,3,\bfs{t}}(y) \hdots, B_{q,3,\bfs{t}}(y))^{\top}$, and $\bfs{\gamma}_j = (\gamma_{j,1},\gamma_{j,2}, \hdots, \gamma_{j,q})^{\top}$.
- But it is not necessary monotonic on $y$. But, it is sufficient for the derivative of the function with respect to $y$ to be greater than zero for all $y $.
- A positive and increasing sequence of all parameters $\gamma_i$ produces a monotonically increasing spline function. 
- We consider an alternative parameterization of the spline parameters to produce monotonically increasing spline function `r Citep(myBib,"pya2015shape")`. 
-  The vector $\bfs{\gamma}_j = (\gamma_{j,1}, \hdots, \gamma_{j,q})^{\top}$ is defined in terms of unconstrained parameters $\bfs{\beta}_j$ as

$$\begin{equation}\label{beta_mono}
  \gamma_{j,1} = \beta_{j,1}, \hspace{0.1cm} \text{and} \hspace{0.1cm} \gamma_{j,l} = \beta_{j,1} + \sum_{i = 2}^l \exp(\beta_{j,i}), \hspace{0.1cm} \text{for} \hspace{0.1cm} l = 2, \hdots, q, 
\end{equation}$$
---

- In the matrix notation we have $\boldsymbol{\gamma}_j = \Sigma \boldsymbol{\tilde{\beta}}_j$, where

$$\begin{equation}\label{Sigma_mono}
\Sigma = \left(
\begin{array}{ccccc}
    1 & 0 & 0 & \dots & 0 \\
    1 & 1 & 0 & \dots & 0 \\
    1 & 1 & 1 & \dots & 0 \\
    \dots & \dots & \dots & \dots & \dots \\
    1 & 1 & 1 & \dots & 1 \\
\end{array}  \right)  
\end{equation}$$

is an $l \times l$ matrix, and 

$$\begin{equation}
    \boldsymbol{\tilde{\beta}}_j = (\beta_{j,1}, \exp(\beta_{j,2}), \dots, \exp(\beta_{j,q}))^\top.
\end{equation}$$



---

- A further case that contemplates $y$ and $x$ non-linear interaction is achieved by considering the tensor product of B-splines bases.
- We use two distinct B-spline bases for $y$ and $x $, each one with of degree $k$ with knot vectors $\mathbf{t}_1$ and $\mathbf{t}_2 $, respectively
- Let us take two distinct B-spline basis for $y$ and $x$ of degree $k$ with knots vector $\boldsymbol{l}_1$ and $\boldsymbol{l}_2$. 
-  The transformation function formed by the tensor product of these bases as $h_j(y|x)$ and express it as

$$\begin{align}\label{bctm_model}
    h_j(y|x) = \sum_{l_1 = 1}^{L_1}\sum_{l_2 = 1}^{L_2} \gamma_{j,l_1,l_2} B_{l_1,3,\boldsymbol{t}_1}(y)B_{l_2,3,\boldsymbol{t}_2}(x) =  (\boldsymbol{a}(y)^{\top} \otimes \boldsymbol{b}(x)^{\top}) \boldsymbol{\gamma}_j \end{align}$$

- We have a total of $L_1 \times L_2$ parameters.
- Now, the restriction of monotonicity is necessary only in the $y$ direction. In the matrix notation $\boldsymbol{\gamma}_j = \Sigma \boldsymbol{\tilde{\beta}}_j$ with $\Sigma = \Sigma_1 \otimes I_2$ and

$$\begin{equation}\label{betatilde}
    \bfs{\tilde{\beta}}_j = (\beta_{j,1,1}, \beta_{j,1,2}, \hdots, \beta_{j,1,L_2}, \exp(\beta_{j,2,1}), \exp(\beta_{j,2,2}), \hdots, \exp{(\beta_{j,2,L_2})}, \hdots, \exp(\beta_{j,L_1,L_2}))^{\top} 
\end{equation}$$

---
### Prior specification and P-splines
___

- The B-spline bases is already used in regression models. In this context, they estimate effects of exploratory variables using smooth functions.
- `r Citep(myBib,"eilers1996flexible")` proposes an appealing penalty based on the finite difference of adjacent basis coefficients, called P-splines to addresses the overfitting problem of unregularized splines.
- In the Bayesian context, smoothness and regularization are achieved through shrinkage priors distributions.
- In the Bayesian P-Splines approach `r Citep(myBib,"lang2004bayesian")`, we assign multivariate normal distributions with a zero vector mean and a precision matrix $\boldsymbol{Q}$ to the parameter models
---
- For the case $h_j(y) = \sum_{i = 1}^q B_{i,3,\bfs{t}}(y) \gamma_{j, i} =  \boldsymbol{a}(y)^T \bfs{\gamma}_j$, see (\ref{spline}),  we set $\bfs{\beta}_j \sim N_q(0, \bfs{Q}_j(\tau_j))$, which $\bfs{Q}_j(\tau_j) = \frac{1}{\tau_j}\bfs{K}_1$, and


$$\begin{equation}\label{ordem1}
\mathbf{K}_1 =
\begin{bmatrix}
1 & -1 & 0 & 0 & \cdots & 0 \\
-1 & 2 & -1 & 0 & \cdots & 0 \\
0 & -1 & 2 & -1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & -1 & 2 & -1 \\
0 & 0 & 0 & 0 & -1 & 1
\end{bmatrix}_{q \times q}
\end{equation}$$

\noindent which is $q - 1$ rank. Then the prior distribution for $\bfs{\beta}$ is improper. To guarantee that the prior for $\bfs{\beta}_j$ will be proper, we sum $10^{-6}$ in diagonal elements to ensure the positive definiteness of $\bfs{K}_1$.
- The penalty matrix $\bfs{K}_1$ enforces that the unconstrained parameters stay close to each other. promoting smoothness in the estimated function.
- By penalizing differences between adjacent parameters, it helps prevent overfitting and ensures a more stable and regularized solution.

---
- The hyperparameter $\tau_j$ is represented internally on the log scale, that is, $\theta_j = \log(\tau_j)$. This representation is computationally convenient because the parameter is not bounded on this scale.

-  We set a Half-Cauchy distribution with location parameter $x_0 = 0$ and scale $\varphi = 25$ as the prior distribution for $\tau_j$.

$$\begin{equation}
    \pi(\theta) = \pi_{\tau}(\exp(\theta)) \left| \frac{\partial g^{-1}(\theta)}{\partial\theta} \right| \\
    =\pi_{\tau}(\exp(\theta)) \exp(\theta) 
\end{equation}$$

- The amount of smoothness induced by the precision matrix (\ref{ordem1}) is controlled by the hyperparameter.

- In the tensor product model for $h_j(y|\boldsymbol{x})$, we maintain a Gaussian prior distribution for $\boldsymbol{\beta}_j$ parameters. Although the precision matrix controls the smoothness in two directions

$$\begin{equation}\label{priori_K_2}
    Q_j(\bfs{\tau}_j) = \frac{1}{\tau_{j1}} \left(\bfs{K_1} \otimes I_{L_2}\right) +
    \frac{1}{\tau_{j2}}\left(I_{L_1} \otimes \bfs{K_2}\right),
\end{equation}$$

---

THe $\bfs{K}_2$ matrix considered here \citep{carlan2023bayesian}
can be represented as $\bfs{K}_2 = \bfs{P}^T \bfs{P}$, where

$$\begin{equation}\label{matrixP}
\bfs{P} = \left(
    \begin{array}{ccccc}
        \bfs{P}_{u} & \bfs{0} & \bfs{0} & \dots & \bfs{0} \\
         \bfs{0} & \bfs{P}_s & \bfs{0} & \dots & \bfs{0} \\
         \bfs{0} & \bfs{0} & \bfs{P}_{s} & \dots & \bfs{0} \\
         \dots & \dots & \dots & \ddots & \vdots \\
          \bfs{0} & \bfs{0} & \bfs{0} & \dots & \bfs{P}_s
    \end{array} \right),
\end{equation}$$

\noindent $\bfs{0}$ are null matrices of the corresponding dimensions. The $\boldsymbol{P}_u$ and $\boldsymbol{P}_s$ matrices are the $(L_2 - 2)\times L_2$ and $(L_2 - 1)\times L_2$ matrices given by


$$\begin{equation} \bfs{P}_u = \left(
    \begin{array}{cccccc}
        1 & -2 & 1 & 0 & \dots & 0 \\
        0 & 1 & -2 & 1 &  \dots & 0 \\
        0 & 0 & 1 & -2 &  \dots & 0 \\
        \dots & \dots & \dots  & \dots &  \ddots & \vdots \\
        0 & 0& 0 & 0&  \dots & 0 \\
    \end{array}\right), \bfs{P}_s = \left(\begin{array}{ccccc}
        -1 & 1 & 0 & \dots & 0 \\
         0 & -1 & 1 & \dots & 0 \\
         \dots & \dots& \dots& \ddots &\vdots \\
         0 & 0 & 0 & \dots & 1 \\
    \end{array}\right).
\end{equation}$$



- For the vector $\boldsymbol{\tau} = (\tau_1, \tau_2)$ we set independent prior distributions considering then $\pi(\boldsymbol{\tau}) = \pi(\tau_1)\pi(\tau_2)$.
---
### Bernstein polynomial basis
___

**Definition:** Let $k$ be a non-negative integer and $t \in [0,1]$, the Bernstein polynomials of degree $k$ are defined by

\begin{equation}\label{bern_def}
    B_{r,k}(t) = \binom{k}{r} t^r(1 - t)^{k -r},
\end{equation}

\noindent for $r = 0, \hdots, k$,  where 

$$
\binom{k}{r} = \frac{k!}{r!(k-r)!}.
$$

 The Bernstein polynomials of degree 1 are represented by the following equations:  

$$\begin{align}
    B_{0,1}(t) & = 1 - t, \\
    B_{1,1}(t) & = t,
\end{align}$$

\noindent for $t \in [0,1]$.  

The Bernstein polynomials of degree 2 are given by:  

$$\begin{align}
    B_{0,2}(t) & = (1 - t)^2, \\
    B_{1,2}(t) & = 2t(1 - t), \\
    B_{2,2}(t) & = t^2,
\end{align}$$

\noindent also for $t \in [0,1]$. 

---

- These polynomials can be combined to approximate or represent a function as follows:  

$$\begin{equation}\label{func_asbern}
    f(x) = \sum_{r=0}^k B_{r,k}(x) \beta_r,
\end{equation}$$

\noindent where $\beta_r$, $r = 1, \dots, k,$ are the Bernstein coefficients. 

- The domain of the Bernstein basis can be redefined over an arbitrary interval $[a, b]$ by changing the variable $y = (b-a)t + a$ for $t \in [0,1]$, resulting in:  
   $$B_{r,k}(y) = \binom{k}{r} \left(\frac{y-a}{b-a}\right)^r \left(1 - \frac{y-a}{b-a}\right)^{k-r}, \quad y \in [a,b].$$  
    
- We propose to represent the transformation function $h_j(y)$, independent of explanatory variables, $\boldsymbol{x}$, for some $j = 1, \hdots, J$ as a Bernstein polynomial of degree $k$

$$\begin{equation}\label{bern_transf}
    h_j(y) = \sum_{r = 0}^p B_{r,k}(y) \gamma_{j,k+1} = \boldsymbol{a}_j(y)^{\top} \boldsymbol{\gamma}_j
\end{equation}$$
---

** Proposition ** Let $h_j(y)$ be the transformation function defined in (\ref{bern_transf}) with Bernstein basis representation. Let $\boldsymbol{\gamma}_j = \boldsymbol{\Sigma}_j \boldsymbol{\Tilde{\beta}}_j$ with $\boldsymbol{\Tilde{\beta}}_j$ as in (\ref{beta_tilde}) and $\Sigma$ defined in (\ref{Sigma_mono}). Then, $h_j(y)$ is monotonically increasing, that is for all $y_1, y_2 \in \mathbb{R}$ with $y_1 < y_2$ we have $h(y_1) \leq h(y_2)$.


---
### Properties of the BCTM

- Let us assume the transformation function $h(y|\boldsymbol{x}) = \sum_{j = 1}^J h_j(y|\boldsymbol{x})$, and a set of explanatory variables represented by $\boldsymbol{x} = (x_1, x_2, \hdots, x_m)$. The marginal impact of a particular covariate $x_l \in \bfs{x}, l =1, \hdots, m$ is the expected rate of change on the expected value of $Y$ for a one-unit variation in the covariate $x_l$, keeping all other variables constant is defined by


$$\begin{align}\label{rate_of_chane_ey}
    \frac{\mathbb{E}(Y|\bfs{X} = \bfs{x})}{\partial x_l}  = &\int_{\mathbb{R}} y \frac{\partial }{\partial x_l} f_Z(h(y|\boldsymbol{x}))h'(y|\boldsymbol{x}) dy  \nonumber \\
     = & \int_{\mathbb{R}} y \left[ f_Z(h(y|\boldsymbol{x}))\left( 2 h(y|\boldsymbol{x}) \frac{\partial h(y|\boldsymbol{x})}{\partial x_l}h'(y|\boldsymbol{x}) + \frac{\partial h'(y|\boldsymbol{x})}{\partial x_l} \right)\right] dy
\end{align}$$

- If there is no interaction between $y$ and $x_l$ in the terms of $h(y|\boldsymbol{x})$ the expression in (\ref{rate_of_chane_ey}) simplifies to

$$\begin{align}\label{impact_EY_nointeraction}
    & = \int_{\mathbb{R}} y \left[ f_Z(h(y|\boldsymbol{x}))\left( 2 h(y|\boldsymbol{x}) \frac{\partial h(y|\boldsymbol{x})}{\partial x_1}h'(y|\boldsymbol{x}) \right)\right] dy \nonumber \\
    & =  2 \frac{\partial h(y|\boldsymbol{x})}{\partial x_1} \mathbb{E}(Yh(Y|\boldsymbol{x})).
\end{align}$$

---
- If even the term of $x_l$ in the transformation function $h(y|\boldsymbol{x})$ is linear, that is, $h_j(x_l) = x_l\beta_j$ for some $j$ from 1 to $J$, and the coefficient $\beta_j \in \mathbb{R}$ the effect of $x_l$ is not linear on the expected value of $Y$



$$\begin{align}
    \frac{E(Y|\bfs{X} = \bfs{x})}{\partial x_l} = &  2\beta_j \int y h(y|\boldsymbol{x}) \left[ f_{Y|\boldsymbol{x}}(y) \right] dy \nonumber \\
    & =  2\beta_j \mathbb{E}(Yh(Y|\boldsymbol{x})). \nonumber   
\end{align}$$

**Property** Let $Q_p$ be the p-quantile of the random variable $Y|\boldsymbol{X} = \boldsymbol{x}$. Then,

$$\begin{equation}
    Q_p = h^{-1}(\Phi^{-1}(p)|\boldsymbol{x})
\end{equation}$$

- Let us assume the transformation function $h(y|\boldsymbol{x}) = \sum_{j = 1}^J h_j(y|\boldsymbol{x})$, and a set of explanatory variables represented by $\boldsymbol{x} = (x_1, x_2, \hdots, x_p)$. The marginal impact of a particular covariate $x_l \in \bfs{x}, l =1, \hdots, m$ is the expected rate of change on the quantile $Q_p$ of $Y|\bfs{X} = \bfs{x}$ for a one-unit variation in the covariate $x_l$, keeping all other variables constant


$$\begin{align}\label{rate_of_chane_qp}
    \frac{\partial Q_p}{\partial x_l} = & \left[ \frac{\partial h(q_p \mid \boldsymbol{x}) - \Phi^{-1}(p)}{\partial x_l} \right] \left[ \frac{\partial h(q_p \mid \boldsymbol{x})}{\partial q_p} \right]^{-1} \quad \text{(using the implicit differentiation rule)} \nonumber \\
    = & \left[ \sum_{j = 1}^J \frac{\partial h_j(y \mid \boldsymbol{x})}{\partial x_l} \right] \left[ \frac{\partial h(q_p \mid \boldsymbol{x})}{\partial q_p} \right]^{-1}.
\end{align}$$

---
If even the term of $x_l$ in the transformation function $h(y|\boldsymbol{x})$ is linear, that is, $h_j(x_l) = x_l\beta_j$ for some $j$ from 1 to $J$, and the coefficient $\beta_j \in \mathbb{R}$ the effect of $x_l$ is not linear on the quantile $p$ of $Y$

$$\begin{equation}
    \frac{\partial q_p}{\partial x_l}  = \beta_j \left[ \frac{\partial h(q_p|\boldsymbol{x})}{\partial q_p} \right]^{-1} .
\end{equation}$$


---
### Bayesian Count CTM

- The primary goal of the proposed model is to directly estimate the discrete conditional probability distribution $F_{Y \mid \bfs{X} = \bfs{x}}$, where now $Y \in \{ 0, 1, 2, \dots \}$

- The Bayesian count transformation model specifies the conditional distribution function $F_{Y|\bfs{X} = \bfs{x}}$ of a count response $Y$ given $x$ as follows:  

$$\begin{equation}\label{count_BCTM}
F_{Y|X = x}(y) = P(Y \leq y \mid x) = F_Z(h(y|\boldsymbol{x}) = F_Z(\alpha( \lfloor y \rfloor) + h(\bfs{x})), \quad y \in \mathbb{R}^+,
\end{equation}$$

- where $\alpha$ is a smooth, continuous, and monotonically increasing function applied to the greatest integer $\lfloor y \rfloor$ less than or equal to $y$. The function $\alpha$ can be written in terms of basis functions $\bfs{a}: \mathbb{R} \rightarrow \mathbb{R}^q $, such that


\begin{equation}
    \alpha(y) = \bfs{a}(y)^{\top} \boldsymbol{\gamma}, \bfs{\gamma} \in \mathbb{R}^q,
\end{equation}
 
\noindent where $\bfs{a}(y)$ is a vector of evaluated Bernstein or B-spline basis functions. The restrictions on $\bfs{\gamma}$ are those already presented. As a consequence, the function $\alpha(y)$ is monotonically increasing in $y$.

---
- For this count model, we restrict the transformation function to $h(y \mid \bfs{x}) = \alpha(y) + h(\bfs{x})$, where $h(\boldsymbol{x})$ can be decomposed as
- The functions in (\ref{hx}) could include linear fixed effects, $h_j(x_i) = x_i\beta_j$, for some $i = 1,\hdots, p$, and $j = 1, \hdots, J$, or $h_j(x_i) = \sum \boldsymbol{b}(x_i)\bfs{\beta}_j$ for some evaluated basis (B-spline or Bernstein) of $x_i$. 

$$\begin{equation}\label{hx}
  h(\bfs{x}) = \sum_{j = 1}^J h_j(\boldsymbol{x}). 
\end{equation}$$

- The construction in (\ref{count_BCTM}) is identical to the continuous BCTM; however, the floor function applied to \( y \) makes the transformation function stepwise, with jumps at the integers \( 0, 1, 2, \dots \).

$$\begin{equation}\label{count_BCTM}
F_{Y \mid \bfs{X} = \bfs{x}}(y) = F(\alpha(\lfloor y \rfloor) + \bfs{x}^\top \bfs{\beta})
\end{equation}$$

---

```{r, echo=FALSE,  out.width = "75%", dpi = 300 , fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/count_model.png")
```

Count transformation model. Illustration of a cumulative distribution function (F, left panel) and of a
transformation function ($h(y|\bfs{x})$, right panel) of a count response ($\lfloor y \rfloor$, red) and a corresponding continuous variable ($y$, blue) both functions coinciding for counts $0,1,2,\hdots$. 

- An important technical detail is that the function $\alpha$ is specified in terms of basis functions of $a(y)$, $a(y+1)$, or even on the log-scale: $a(\log(y))$ or $a(\log(y + 1))$. 

---
### Posterior Inference
___
- From the transformation functions defined so far, we have the parameter models, $\boldsymbol{\gamma}$, both for linear and B-spline transformation functions, and the hyperparameters $\boldsymbol{\tau}$, for B-spline transformation.
- To obtain posterior estimates of the BCTM, `r Citet(myBib,"carlan2023bayesian")` proposed an MCMC method based on No-U-Turn-Sample and Gibbs sampling to sample values from the posterior distribution of model parameters.
- We present an alternative estimation method based on the Integrated Nested Laplace Approximation (INLA) given by `r Citet(myBib,"rue2009approximate")`.
- The contribution is to provide an alternative way to the MCMC methods to implement Bayesian inference on these models.

- In this context, we aim to contribute to the BCTM class by proposing an algorithm that is more computationally efficient than traditional MCMC approaches. This chapter introduces two alternative estimation methods for this class of models, both based on approximation techniques, including Laplace approximation and variational inference.

---

### Integrated Laplace with Bayesian Conditional Transformation Models
___

- In the BCTM, we have the vector of constrained parameters $\bfs{\gamma}$, which are functions the unconstrained vector of parameters $\bfs{\beta}$. 
-  Additionally, we have the vector of hyperparameters $\boldsymbol{\tau} = (\bfs{\tau}_1, \dots, \bfs{\tau}_J)^{\top}$.
- Ultimately, we have the following quantities to estimate: $\bfs{\beta} = (\bfs{\beta}_1, \dots, \bfs{\beta}_J)^{\top}$, and $\boldsymbol{\tau} = (\bfs{\tau}_1, \dots, \bfs{\tau}_J)^{\top}$.
- To obtain posterior estimates of the BCTM, we propose an estimation method based on the Integrated Nested Laplace Approximation (INLA) presented by \citet{rue2009approximate}.
Let $\bfs{y} = (y_1, \ldots, y_n)^{\top}$ be a random sample of the random variable $Y$, which is independently distributed given $\bfs{x}$ with distribution $F_{Y|\bfs{X} = \bfs{x}}$. Then, the joint posterior distribution of the parameters $\boldsymbol{\beta}$ and $\bfs{\tau}$ is

$$\begin{equation}\label{joint_prior}
\pi(\bfs{\beta}, \bfs{\tau}|\bfs{y}) \propto \left[ f_{\bfs{Y}|\bfs{X} = \bfs{x}}(\bfs{y}|\bfs{x}, \bfs{\beta})\right] 
    \phi(\bfs{\beta}|\bfs{\mu}, \bfs{Q}(\bfs{\tau})) \pi(\bfs{\tau}), 
\end{equation}$$



---

- We are interested in the marginal distribution of the unconstrained parameters $\beta_r$, which are components of vector $\bfs{\beta}_j = (\beta_1, \hdots, \beta_{I_j})^{\top}$ from the vector $\boldsymbol{\beta} = (\bfs{\beta}_1, \hdots, \bfs{\beta}_J)^{\top}$, where $j \in \lbrace 1, \dots, J\rbrace$,

$$\begin{equation}\label{marginal_app}
    \pi(\beta_r|\bfs{y}) = \int \pi(\beta_r|\bfs{\tau},\bfs{y}) \pi(\bfs{\tau}|\bfs{y})d\bfs{\tau}.
\end{equation}$$

- The marginal $\pi(\beta_i|\boldsymbol{y})$ involves
  - approximation of $\pi(\boldsymbol{\tau}|\boldsymbol{y})$.
  - approximation of $\pi(\beta_{r}|\boldsymbol{\tau},\boldsymbol{y})$.
  - a numerical integration.


---
-  The first step is approximate $\pi(\boldsymbol{\tau}|\boldsymbol{y})$ which is given by
 
$$\begin{equation}\label{hyper_dist}
   \tilde{\pi}(\boldsymbol{\tau}|\boldsymbol{y}) \propto  \frac{\pi(\boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{y})}{\tilde{\pi}(\boldsymbol{\beta}| \boldsymbol{\tau}, \boldsymbol{y})}\bigg|_{\boldsymbol{\beta} = \boldsymbol{\beta}^{*}(\boldsymbol{\tau})}
\end{equation}$$

- The approximation from the denominator comes from the fact that we can write

$$\begin{align}\label{condi_beta}
    \pi(\boldsymbol{\beta}|\boldsymbol{\tau}, \boldsymbol{y}) = & \frac{\pi(\boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{y})}{\int \pi(\boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{y}) d\boldsymbol{\beta}} \nonumber \\    
    = & \frac{\exp(p(\boldsymbol{y}|\boldsymbol{\beta}) + p(\boldsymbol{\beta}| \boldsymbol{\tau}) + p(\tau))}{\int \exp(p(\boldsymbol{y}|\boldsymbol{\beta}) + p(\boldsymbol{\beta}| \boldsymbol{\tau}) + p(\tau))d\boldsymbol{\beta}} \nonumber \\
    = & \frac{\exp(p(\boldsymbol{\beta}))}{\int \exp(p(\boldsymbol{\beta})) d\boldsymbol{\beta}}
\end{align}$$

- We write $p(\bfs{\beta}) = \log \pi(\bfs{\beta}, \bfs{\tau}, \bfs{y}) $ (the conditional on $\bfs{\tau}$ and $\bfs{y}$ is implicit). 

$$\begin{align}\label{pbeta}
    p(\bfs{\beta}) = & \log \pi(\bfs{y}|\bfs{\beta}) + \log \pi(\bfs{\beta}| \bfs{\tau}) + \log \pi(\bfs{\tau}) \nonumber \\
    p(\bfs{\beta}) = & \sum_{i = 1}^n \left[ \log f_Z(\bfs{c}(y_i,\bfs{x}_i)^{\top}\bfs{\gamma(\beta)}) + \log (\bfs{c}'(y_i,\bfs{x}_i)^{\top}\bfs{\gamma(\beta)})\right] \nonumber \\
    & -0.5 \bfs{\beta}^{\top} \bfs{Q}(\bfs{\tau}) \bfs{\beta} + \sum_{j = 1}^J\sum_{l \in \mathcal{L}_j} \beta_{lj} + \log \pi(\bfs{\tau}).
\end{align}$$

---

- The integral in the denominator of $\pi(\boldsymbol{\beta}|\boldsymbol{\tau}, \boldsymbol{y})$ can be approximated by Laplace  method

$$\begin{equation}
    \int \exp(p(\boldsymbol{\beta})) d\boldsymbol{\beta} \approx  \exp(p(\boldsymbol{\beta}^{*}))(2\pi)^{I/2} 
    | -H_p(\boldsymbol{\beta}^{*}(\boldsymbol{\tau}))|^{-1/2}
\end{equation}$$
where $\boldsymbol{\beta}^{*}(\boldsymbol{\tau})$ is the mode of $p(\boldsymbol{\beta})$ for a given $\boldsymbol{\tau}$, and $H_p(\boldsymbol{\boldsymbol{\beta}^{*}(\boldsymbol{\tau})})$ is the Hessian matrix of $p(\boldsymbol{\beta})$ evaluated at the mode. 

---

- The gradient and Hessian matrix have a analytical expressions!
- To evaluate the approximation in (\ref{laplace_approx}), we have to optimize $p(\bfs{\beta})$ to find the mode $\bfs{\beta^*(\tau)}$ for a given $\bfs{\tau}$. We applied the following Newton-Raphson method. 
- The final approximation of log density of full conditional evaluated at the mode $\boldsymbol{\beta}^{*}(\boldsymbol{\tau})$  is
$$\begin{equation}
    \log( \tilde{\pi}(\boldsymbol{\beta}^{*}(\boldsymbol{\tau})|\boldsymbol{y}, \boldsymbol{\tau})) \approx -\frac{n}{2} \log(2\pi) + \frac{1}{2} \log \left| -H_p(\boldsymbol{\beta}^{*}(\boldsymbol{\tau})) \right |
\end{equation}$$
- Finally, the approximated distribution $\tilde{\pi}(\boldsymbol{\tau}|\boldsymbol{y})$ is given by

$$\begin{equation}
    \log \tilde{\pi}(\boldsymbol{\tau}|\boldsymbol{y}) \propto \left[ \log(\pi(\boldsymbol{y},\boldsymbol{\beta}^{*}(\boldsymbol{\tau}), \boldsymbol{\tau}) - \frac{1}{2} \log \left| -H_p(\boldsymbol{\beta}^{*}(\boldsymbol{\tau})) \right| \right]_{\boldsymbol{\beta} = \boldsymbol{\beta}^{*}(\boldsymbol{\tau})}
\end{equation}$$


- The difficult is that the evaluation of this distribution require the optimization of $p(\boldsymbol{\beta})$.


---

### Exploring hyper-parameter distribution
___
- Locate the $\boldsymbol{\tau}^{*}$ mode by using the quasi-Newton L-BFGS-B method to optimize the $\log \pi(\boldsymbol{\tau}|\boldsymbol{y})$ density concerning $\boldsymbol{\tau}$.

- Select and evaluate a grid of points around $\boldsymbol{\tau}^{*}$, like $\boldsymbol{\tau}^{*} \pm \delta$, where $\delta$ parameter can be customized. We adopted small values like 1.

.pull-left[
```{r, echo=FALSE,  out.width = "70%", dpi = 300 , fig.cap="Approximated distribution for one hyperparameter.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Hyper_uni_den.png")
```

]

.pull-right[
```{r, echo=FALSE,  out.width = "70%", , dpi = 300 , fig.cap="Approximated distribution for two hyperparameter.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Den_hyper.png")
```
]
---
#### Approximating $\pi(\beta_r|\bfs{\tau}, \bfs{y})$

- The next step is to approximate the posterior marginals for the $\beta_r$'s from $\bfs{\beta}$ vector, conditioned on $\bfs{\tau}$.
- Using the same logical of the approximation in (\ref{hyperdist1}) we got the following equation 

$$\begin{equation}\label{beta_marginais}
    \Tilde{\pi}({\beta}_r| \bfs{\tau}, \bfs{y}) \propto \frac{\pi(\bfs{\beta}, \bfs{\tau}, \bfs{y})}{\Tilde{\pi}(\bfs{\beta_{-r}} |\beta_r, \bfs{\tau}, \bfs{y})} \bigg|_{\bfs{\beta}_{-r} = \bfs{\beta}_{-r}^{*}(\beta_r, \bfs{\tau})},
\end{equation}$$
 where $\bfs{\beta}_{-r}^{*}(\beta_r, \bfs{\tau})$ maximizes $p(\bfs{\beta}) \propto \log(\pi(\bfs{\beta}, \bfs{\tau},\bfs{y}))$ given the constraint $\beta^*(\bfs{\tau})_r = \beta_r$ (the r-th component of the mode vector $\beta^*(\bfs{\tau})$ is equal to $\beta_r$), and $\Tilde{\pi}(\beta_{-r} |\beta_r, \bfs{\tau}, \bfs{y})$ is an approximation to $\beta_{-r} |\beta_r, \bfs{\tau}, \bfs{y}$ evaluated at $\bfs{\beta}_{-r}^{*}(\beta_r, \bfs{\tau})$.
 
 
- The denominator in \ref{beta_marginais} is a approximation from 

$$\begin{equation}\label{beta_marginais2}
    \pi(\boldsymbol{\beta}_{-i}|{\beta}_{i}, \boldsymbol{\tau},\boldsymbol{y}) = \frac{\pi(\boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{y})}{\int \pi(\boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{y})d\boldsymbol{\beta}_{-i}}
\end{equation}$$
- The computational cost to approximate (\ref{beta_marginais2}) is massive since the Laplace method for the integral in the denominator involves a constrained optimization of $\bfs{\beta}_r$ for a set of values of $\beta_r$ for all $r = 1, \hdots I_j \times J$, with $I_j$ being the number of parameters of a particular transformation $j = 1, \hdots, J$.

---
- A computational cheaper option is to base this approximation on a conditional density implied by the Gaussian approximation, $\tilde{\pi}_G(\boldsymbol{\beta}| \boldsymbol{\tau}, \boldsymbol{y})$, for $\boldsymbol{\beta}| \boldsymbol{\tau}, \boldsymbol{y}$ distribution.
- The mode $\boldsymbol{\beta}_{-i}$ now would be the conditional mean of a multivariate normal distribution given by
$$\begin{equation}\label{mode_condi}
    \boldsymbol{\beta}_{-i}^{*}(\beta_i, \boldsymbol{\tau}) = E_{\tilde{\pi}_G(\boldsymbol{\beta}_{-i} |\beta_i)} = \boldsymbol{\beta}(\boldsymbol{\tau})_{-i} + (H_{-i,i})^{-1}H_{i,i}\left[\beta_i -  \boldsymbol{\beta}(\boldsymbol{\tau})_{i}\right].
\end{equation}$$
where $\boldsymbol{\beta}(\boldsymbol{\tau})$ is the mode vector of $p(\boldsymbol{\beta})$ distribution, and $\boldsymbol{\beta}(\boldsymbol{\tau})_{-i}$ is the vector without the ith component. The conditional Hessian is given by
$$\begin{equation}
    H_{\tilde{\pi}_G(\boldsymbol{\beta}_{-i} |\beta_i)} = H_{-i,-i} - (H_{-i,i})^{-1}H_{i,i}(H_{i,-i}),
\end{equation}$$
where $H$ is the Hessian from $p(\boldsymbol{\beta})$ applied to $\boldsymbol{\beta}(\boldsymbol{\tau})$, it is constant since is independent of $\beta_i$.
- Since the Hessian matrix is independent of $\beta_i$ then $\pi(\beta_i| \boldsymbol{\tau}, \boldsymbol{y}) \propto \pi(\tilde{\boldsymbol{\beta}}, \boldsymbol{y}, \boldsymbol{\tau})$. 
- The benefits of this approximation are immediate since we do not evaluate the integral in (\ref{beta_marginais2}) for each $i$ but only to find the mode of $p(\boldsymbol{\beta})$ and by Normal distribution properties find the conditional expected value.


---


- The last step is to integrate $\boldsymbol{\tau}$ from $\pi(\beta_i| \boldsymbol{\tau}, \boldsymbol{y})$ numerically as 

$$\begin{equation}
  \int \pi(\beta_i| \boldsymbol{\tau}, \boldsymbol{y}) d\boldsymbol{\tau} = \sum_{j = 1}^{J} \pi(\beta_i| \boldsymbol{\tau}^{(j)}, \boldsymbol{y}) \Delta_j.   
\end{equation}$$
- Finally, we get a approximation of the marginal by getting a set of evaluated density points.
- The challenge of the algorithm is that the whole process of evaluated $\beta$ marginal's has to be repeated for each $\tau$ value.
- For multivariate case we have a lot of points.

---
### Variational Bayes
___

- an alternative approach for posterior inference in the Bayesian Conditional Transformation models based on Variational Bayes (VB).

- Variational Bayes (VB) é uma técnica de inferência aproximada usada para estimar distribuições a posteriori em modelos bayesianos complexos. Ele é especialmente útil quando a distribuição a posteriori exata é intratável ou computacionalmente cara de obter.

- Há muitos algoritmos de VB disponiveis. O algoritmo apresentado \citet{HaavardVBC} é uma proposta da classe de algoritmos VB. A proposta original tem como objetivo ser uma alternativa ao INLA para a classe de Latent Gaussian Models (LGM).

- O algoritmo foi nomeado de "", pois unknown density function using a Gaussian approximation via the Laplace method, followed by a variational correction applied to the mode of this Gaussian approximation.

- Nossa proposta é adaptar esse algortimo para a classe BCTM. Isso é plausivel visto que nossa primeira proposta já envolvia aproximações normais para as posterioris de interesse.

---
- VB gained popularity due to the increasing complexity of Bayesian models, the massive amount of analyzed data,  and the growing need for computationally efficient alternatives to exact and sampling-based methods.

- The VB methods optimize an objective function over a specific family of distributions to approximate the true posterior distribution in Bayesian inference.

- Unlike Gaussian approximations, VB methods are more general, as the chosen family of distributions is not limited to Gaussian ones. If the selected family includes the true posterior, the VB approximation can recover the exact posterior.

 **A ideia central do VB é transformar o problema da inferência bayesiana em um problema de otimização.**
 
 - Dada uma distribuição a posteriori p(θ∣D), que geralmente é complexa, o VB busca encontrar uma aproximação q(θ) que esteja próxima de p(θ∣D)

---

- This posterior (\( \pi(\bfs{\theta}|y) \)) can be defined through the minimization of the following specific objective function


\begin{equation}\label{opt_principal}
    \pi^*(\boldsymbol{\theta}|\bfs{y})= \underset{q(\boldsymbol{\theta}) \in \mathcal{P}(\boldsymbol{\Theta)}}{\textrm{arg min}} \left\lbrace E_{q(\boldsymbol{\theta})} \left[\sum_{i = 1}^n l(\boldsymbol{\theta};y_i) \right] + \textrm{KLD}(q(\boldsymbol{\theta})||\pi(\boldsymbol{\theta})) \right\rbrace,
\end{equation}

- A posteriori encontrada em () é a mesma que:
  - Minimiza a divergência de KL entre a posteriori verdadeira e a estimada.
  - A que maximiza a ELBO (Evidence Lower BOund)
- However, working on an infinite domain is impractical. Therefore, the VB methods constrain the posterior distribution to a parametric form, $q(\boldsymbol{\theta}) \in \mathcal{Q}$, where $\mathcal{Q} = \lbrace q(\boldsymbol{\theta}| \boldsymbol{\kappa}): \boldsymbol{\kappa} \in \mathcal{K}\rbrace $ is a family of distributions on $\boldsymbol{\Theta}$ parameterized by $\boldsymbol{\kappa}$.
-  We call $\mathcal{Q}$ as Variational family, and it is the space where the best approximated posterior distribution is searched.

---
This best posterior from this parametric family is the solution of equation (\ref{opt_principal}), but $\mathcal{Q}$ instead of $\mathcal{P}(\Theta)$, that is 

\begin{equation}\label{pract_opt}
    \pi_{{\small VB}}^*(\boldsymbol{\theta}|\bfs{y}) = \underset{q(\boldsymbol{\theta}|\boldsymbol{k}) \in \mathcal{Q}}{\textrm{arg min}} \left\lbrace E_{q(\boldsymbol{\theta})} \left[\sum_{i = 1}^n l(\boldsymbol{\theta};y_i) \right] + \textrm{KLD}(q(\boldsymbol{\theta})||\pi(\boldsymbol{\theta})) \right\rbrace,
\end{equation}

where $\pi_{{\small VB}}^*(\boldsymbol{\theta}|\bfs{y}) = q(\boldsymbol{\theta}|\boldsymbol{k}^*)$ for some optimal parameter $\boldsymbol{k}^* \in \mathcal{K}$.

```{r, echo=FALSE,  out.width = "35%", , dpi = 300 , fig.cap="Approximated distribution for two hyperparameter.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/interpretacao2VB.png")
```

---
### Variational Bayes correction

- A recent contribution to VB is the variational correction, where the VB is not employed as an approximation for an unknown posterior, but as a correction to the Gaussian approximation of the posterior distribution obtained from the Laplace method.

- The Gaussian approximation of the log posterior distribution, $\pi(\boldsymbol{\theta}|\boldsymbol{y})$, derived from the Laplace method \citep{tierney1986accurate} is given by

\begin{equation}
    \ln(\pi(\boldsymbol{\theta}|\boldsymbol{y})) = \ln \pi(\boldsymbol{\theta}_0) - \frac{1}{2} (\boldsymbol{\theta}- \boldsymbol{\theta}_0)^{\top} \boldsymbol{H}_0(\boldsymbol{\theta}- \boldsymbol{\theta}_0) + \textrm{high order terms,}
\end{equation}

\noindent where $\boldsymbol{\theta}_0$ is the mode of $\ln(\pi(\boldsymbol{\theta}|\boldsymbol{y}))$ and $\boldsymbol{H}_0$ is the negative Hessian matrix of $\ln(\pi(\boldsymbol{\theta}|\boldsymbol{y}))$ evaluated at $\boldsymbol{\theta}_0$. Then the approximation of the posterior distribution is

\begin{equation}\label{gaussian_implied_lm}
    \Tilde{\pi}_{G}(\boldsymbol{\theta}|\boldsymbol{y}) \propto \exp\left( - \frac{1}{2} (\boldsymbol{\theta}- \boldsymbol{\theta}_0)^{\top} \boldsymbol{H}_0(\boldsymbol{\theta}- \boldsymbol{\theta}_0) \right),
\end{equation}


\noindent so that $\boldsymbol{\theta}|\boldsymbol{y} \approx N(\boldsymbol{\theta}_0, \boldsymbol{H}_0^{-1})$
---

- The variational correction proposal is to correct only the mean of this Normal approximation $\pi_{G}(\boldsymbol{\theta}|\boldsymbol{y})$ to achieve a more accurate estimate.

The updated mean is given by $\boldsymbol{\theta}_1 = \boldsymbol{\theta}_0 + \boldsymbol{\delta}$,  where $\boldsymbol{\delta}$ is the correction to the mean. The posterior of $\boldsymbol{\theta}$ is now given by

$$\tilde{\pi}_{G}(\boldsymbol{\theta}|\boldsymbol{y}) \propto \exp\left( - \frac{1}{2} (\boldsymbol{\theta}- \boldsymbol{\theta}_1)^{T} \boldsymbol{H}_0(\boldsymbol{\theta}- \boldsymbol{\theta}_1) \right).$$

- In summary, we need to find a vector $\boldsymbol{\delta}$ of length $m$ that produce a more accurate mean, while fixing the precision matrix based on the Hessian of $\ln(\pi(\bfs{\theta}|\bfs{y}))$. 

- The parametric family $\mathcal{Q}$ for this problem is the multivariate Normal, with known covariance matrix $\bfs{H}_0^{-1}$, and a mean vector $\boldsymbol{\theta}_0 + \boldsymbol{\delta}$.

- To get the best approximation for the posterior distribution from the family $\mathcal{Q}$

$$
    \tilde{\boldsymbol{\delta}} = \underset{\boldsymbol{\delta}} {\textrm{arg min}}\left\lbrace E_{\boldsymbol{\theta} \sim N(\boldsymbol{\theta}_0 + \boldsymbol{\delta}, \boldsymbol{H}^{-1}_0)} \left[-\log \pi(\boldsymbol{y}|\boldsymbol{\theta}) \right] + \textrm{KLD}(\phi(\boldsymbol{\theta}|\boldsymbol{\theta}_0 + \boldsymbol{\delta},\boldsymbol{H}_0^{-1})||\pi(\boldsymbol{\theta})) \right\rbrace,
$$
---

-The proposed approach to approximate the posterior can be summarized as follows:

  -Find the mode $\bfs{\theta}_0$ of $\ln \pi(\boldsymbol{\theta}|\boldsymbol{y})$.

  -Produces the approximation $\boldsymbol{\theta}|\boldsymbol{y} \approx N(\boldsymbol{\theta}_0, \boldsymbol{H}_0^{-1})$, where $\boldsymbol{H}_0$ is the negative Hessian matrix of $\ln(\pi(\boldsymbol{\theta}|\boldsymbol{y}))$ evaluated at $\boldsymbol{\theta}_0$.

  - Solve for $\boldsymbol{\delta}$ the objective function in (\ref{delta_correction1}).

  - The approximated posterior $\boldsymbol{\theta}$ is a multivariate Normal with mean $\boldsymbol{\theta}_1 = \boldsymbol{\theta}_0 + \boldsymbol{\delta}$ and precision matrix $\boldsymbol{Q}_0 = \boldsymbol{H}_0$.


**It is important to note that we are not producing approximate posterior marginals or marginal corrections. Instead, this proposal ensures a joint improvement of the posterior distribution.**

---
### Variational Bayes correction to BCTM

- Our general definition of BCTM involves hyperparameters.

- The variational correction defined up to now does not introduce any hyperparameters into the posterior.

$$\begin{equation}\label{joint_prior2}
  \pi(\bfs{\beta}, \bfs{\tau}|\bfs{y}) \propto \left[ f_{\bfs{Y}|\bfs{X} = \bfs{x}}(\bfs{y}|\bfs{x})\right] 
    \prod_{j = 1}^J \phi_j(\bfs{\beta}_j|\bfs{\mu}_j, \bfs{Q}_j(\bfs{\tau}_j)) \pi(\bfs{\tau}_j).    
\end{equation}$$

- The proposal utilizes the variational correction within the joint Normal posterior $\Tilde{\pi}_G(\boldsymbol{\beta}|\boldsymbol{\tau}, \boldsymbol{y})$.

- Conditional on the hyperparameters, $\boldsymbol{\tau}$,  the corrected posterior mean of the joint distribution is given by $\boldsymbol{\beta}_1(\bfs{\tau}) = \boldsymbol{\beta}_0(\bfs{\tau}) + \boldsymbol{\delta}$, where $\boldsymbol{\delta}$ is the correction term.

- So, now we solve the following (conditionally on $\boldsymbol{\tau}$)

$$\begin{align}\label{delta_correction}
    \Tilde{\boldsymbol{\delta}} &= \underset{\boldsymbol{\delta}} {\textrm{agr min}}\left\lbrace E_{\boldsymbol{\beta}|\boldsymbol{\tau} \sim N(\boldsymbol{\theta}_0 + \boldsymbol{\delta}, \boldsymbol{H}^{-1}_0)} \left[-\log \pi(\boldsymbol{y}|\boldsymbol{\beta},\boldsymbol{\tau} ) \right] + \textrm{KLD}(\phi(\boldsymbol{\beta}|\boldsymbol{\beta}_0 + \boldsymbol{\tau},\boldsymbol{H}_0^{-1})||\phi(\boldsymbol{\beta}| \boldsymbol{0},\boldsymbol{Q}_0)) \right\rbrace, \nonumber \\
    & = \underset{\boldsymbol{\delta}} {\textrm{agr min}}\left\lbrace E_{\boldsymbol{\beta}|\boldsymbol{\tau} \sim N(\boldsymbol{\beta}_0 + \boldsymbol{\delta}, \boldsymbol{H}^{-1}_0)} \left[-\log \pi(\boldsymbol{y}|\boldsymbol{\beta},\boldsymbol{\tau} ) \right] + \frac{1}{2}(\boldsymbol{\beta}_0 + \boldsymbol{\delta})^T \boldsymbol{Q}_0(\boldsymbol{\beta}_0 + \boldsymbol{\delta}) \right\rbrace.
\end{align}$$

---

- hus, the improved Normal approximation to $\pi(\boldsymbol{\beta}|\boldsymbol{y}, \boldsymbol{\tau})$, that we will denote as $\Tilde{\pi}_{VBC}$, has mean $\bfs{\beta}_1$ and precision matrix $\boldsymbol{Q}_0$.

-  From this multivariate improved Normal distribution the marginals distributions, $\Tilde{\pi}_{VBC}(\beta_r|\boldsymbol{y}, \boldsymbol{\tau})$, are Gaussian densities with mean $\beta_{1,r}$ and precision $Q_0^{r,r}$

- Finally, the marginal posteriors of the parameters $\boldsymbol{\beta}$ are calculated as 

$$\begin{equation}
    \Tilde{\pi}_{VBC}(\beta_r|\boldsymbol{y}) = \sum_{p = 1}^P \Tilde{\pi}_{VBC}(\beta_r|\boldsymbol{\tau}^{(p)}, \boldsymbol{y})\Tilde{\pi}(\boldsymbol{\tau}^{(p)}|\bfs{y}) \Delta \bfs{\tau}^{(p)}, 
\end{equation}$$

where $\lbrace \boldsymbol{\tau}^{[1]}, \hdots, \boldsymbol{\tau}^{[P]} \rbrace$ is a set of points of $\boldsymbol{\tau}$.

\begin{align}
    \pi(\boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{y}) & = \prod_{i =1}^n \pi(y_i|\boldsymbol{\beta}, \boldsymbol{\tau})\pi(\boldsymbol{\beta}|\boldsymbol{\tau}) \nonumber \\
    \Tilde{\pi}(\bfs{\tau}|\bfs{y}) & \propto  \frac{\pi(\bfs{\beta}, \bfs{\tau}, \bfs{y})}{\Tilde{\pi}_{G}(\bfs{\beta}| \bfs{\tau}, \bfs{y})}\bigg|_{\bfs{\beta} = \bfs{\beta}^{*}(\bfs{\tau})} \nonumber \\
    \Tilde{\pi}(\beta_r|\boldsymbol{y}) & = \int \Tilde{\pi}_{\small{VBC}}(\beta_r |\boldsymbol{\tau}, \boldsymbol{y})\Tilde{\pi}(\boldsymbol{\tau}|\boldsymbol{y})d\boldsymbol{\tau}
\end{align}

---
### Low-rank correction

- In the proposed algorithm, the correction is applied to the mean of the Normal approximation of the unknown posterior distribution \(\boldsymbol{\beta}|\boldsymbol{\tau}, \boldsymbol{y}\). Specifically, we use the approximation $N(\boldsymbol{\beta}_0 + \boldsymbol{\delta}, H^{-1}_p(\boldsymbol{\beta}_0)),

- The mean \(\boldsymbol{\beta}_0\), prior to correction, is the solution to the following linear system:

\begin{equation}\label{linear_system_beta0}
 H_p(\bfs{\beta}_0) = \nabla p(\bfs{\beta}_0) + H_p(\bfs{\beta}_0) \bfs{\beta}_0,
\end{equation}

- To simplify the notation, let us define $Q_0 = H_p(\boldsymbol{\beta}_0) \quad \text{and} \quad b_0 = \nabla p(\boldsymbol{\beta}_0) + H_p(\boldsymbol{\beta}_0) \boldsymbol{\beta}_0.$

\begin{equation}
    Q_0 \bfs{\beta}_0 = \bfs{b}_0.
\end{equation}

- Given that the number of parameters can grow substantially in some applications, such as random effects models, \citet{HaavardVBC} proposes an implicit correction of the mean term \(\boldsymbol{\beta}_0\) by explicitly correcting the estimated gradient, such that the improved posterior mean \(\boldsymbol{\beta}_1\) satisfies the new linear system:


\begin{equation}
    Q_0 \boldsymbol{\beta}_1 = \boldsymbol{b}_0 + \bfs{\lambda} = \boldsymbol{b}_1,
\end{equation}

- where \(\boldsymbol{\lambda}\) is the implicit correction term applied to \(\boldsymbol{b}_0 = \nabla p(\boldsymbol{\beta}_0) + H_p(\boldsymbol{\beta}_0) \boldsymbol{\beta}_0\). This implies that the mean \(\boldsymbol{\beta}_0\) is no longer explicitly corrected.

---

- However, if \(\boldsymbol{\lambda}\) is of the same dimension as the gradient and the term \(\boldsymbol{\beta}_0\), that is, \(\boldsymbol{\lambda} \in \mathbb{R}^T\), then no computational advantage would be gained over the proposal

- we can examine how a non-zero value of the \(j\)-th element of \(\boldsymbol{\lambda}\), \(\lambda_j \neq 0\), impacts the \(i\)-th element of the corrected mean vector \(\boldsymbol{\beta}_1\), denoted by \(\beta_{1,i}\)

$$\frac{\partial \beta_{1,i}}{\partial \lambda_j} \lambda_j = \frac{\partial \beta_{1,i}}{\partial b_{1,j}} \frac{\partial b_{1,j}}{\partial \lambda_j} \lambda_j = Q_0^{i,j} \lambda_j,$$

- The equation in (\ref{derivada_correcao_low}) shows that the \(j\)-th term of the vector \(\boldsymbol{\lambda}\) impacts the \(i\)-th term of the vector \(\boldsymbol{\beta}_1\) through the term \(Q_0^{i,j}\).

$$\begin{equation}
    \frac{\partial \boldsymbol{\beta}_1}{\partial \lambda_j} \lambda_j = Q_0^{.,j} \lambda_j,
\end{equation}$$

where \(Q_0^{.,j}\) denotes the \(j\)-th column of the matrix \(Q_0\).

---

- In other words, a single term of \(\boldsymbol{\lambda}\) propagates its correction to all elements of \(\boldsymbol{\beta}_1\). 
- This forms the basis of the Low-Rank Variational Bayes correction proposal, as selecting only a subset of p columns of \(Q_0\) would still propagate the correction to all terms of \(\boldsymbol{\beta}_1\).

- Consider a set of indices \( i \in I \subset \{1, 2, \dots, P\} \) for which we aim to correct \( b_{0,i} \). We then extract the corresponding columns of \( \boldsymbol{Q}_0^{-1} \)


$$\begin{equation}
    \boldsymbol{\beta}_1 = \boldsymbol{\beta}_0 + \bfs{Q}_I^{-1}\boldsymbol{\lambda}
\end{equation}$$

- Now, we can optimize (\ref{delta_correction}), but for the $\boldsymbol{\lambda}$ instead of $\boldsymbol{\delta}$ as follows

\begin{equation}
    \Tilde{\bfs{\lambda}} = \underset{\boldsymbol{\lambda}} {\textrm{agr min}}\left\lbrace E_{\boldsymbol{\beta}|\boldsymbol{\tau} \sim N(\boldsymbol{\beta}_0 + \bfs{Q}_I^{-1}\boldsymbol{\lambda}, \boldsymbol{Q}^{-1}_0)} \left[-\log \pi(\boldsymbol{y}|\boldsymbol{\beta},\boldsymbol{\tau} ) \right] + \frac{1}{2}(\boldsymbol{\beta}_0 + \bfs{Q}_I^{-1}\boldsymbol{\lambda})^T \boldsymbol{Q}_0(\boldsymbol{\beta}_0 + \bfs{Q}_I^{-1}\boldsymbol{\lambda}) \right\rbrace.
\end{equation}


---

- There is still no established rule or criterion for selecting the optimal set of indices \(I\).

- In the original paper, \citet{HaavardVBC} proposes selecting indices related to the fixed effects parameters. For a random effects model, for example, all indices related to the random effects of individuals would be excluded.

- However, in our context, we also have the parameters of the transformations of the observed response variable.

- We will not delve further into this discussion in this work. 

---

### Estimation of the conditional cumulative distribution function
___

- The main point of the BCTM is the estimate of conditional distribution. The construction of the conditional distribution involves a non-linear combination of parameters.
- Using the approximated distributions obtained it is complicated to obtain the resultanting distribution of the combination. However, simulations from these marginals are possible by the Metropolis-Hasting (MH) algorithm.
- The resulting MCMC samples are employed to estimate the conditional cumulative distribution $F_{Y|\boldsymbol{X} = \boldsymbol{x}}(y) = \hat{F}_{Y|\boldsymbol{X} = \boldsymbol{x}}(y) = F_Z(\hat{h}(y|\boldsymbol{x}))$ where $\hat{h}(y|\boldsymbol{x})$ is the posterior mean estimate

$$\begin{equation}
     \hat{h}(y|\boldsymbol{x}) = \sum_{s = 1}^S \frac{1}{S}(\boldsymbol{a}_j(y)^{\top} \otimes \boldsymbol{b}_j(x)^{\top})^{\top} \boldsymbol{\gamma}_j^{(s)},
 \end{equation}$$
and the posterior mean estimate for $F_{Y|\boldsymbol{X} = \boldsymbol{x}}(y)$ is 

$$\begin{equation}
    \hat{F}_{Y|\boldsymbol{X} = \boldsymbol{x}}(y) = \frac{1}{S} \sum_{s = 1}^S F_Z((\boldsymbol{a}_j(y)^{\top} \otimes \boldsymbol{b}_j(x)^{\top})^{\top} \boldsymbol{\gamma}_j^{(s)}).
\end{equation}$$


---
### Simulation Study 1: Evaluating the ILBCTM performance
___

- A simulation study was conducted to evaluate the performance of the ILBCTM algorithm in recovering the parameters for some particular cases of BCTM. We restricted ourselves into 3 differents models. 

  - Model 1: $\textrm{P}(Y \leq y) =         \Phi(\boldsymbol{a}(y)^{\top} \boldsymbol{\gamma})$.
  - Model 2: $\textrm{P}(Y \leq y | \bfs{X} = \bfs{x}) = \Phi(\bfs{a}(y)^{\top}\bfs{\gamma} + \bfs{x}^{\top} \bfs{\beta})$.
  - Model 3: $\textrm{P}(Y \leq y | \boldsymbol{X} = \boldsymbol{x}) = \Phi(\boldsymbol{a}(y)^{\top}\boldsymbol{\gamma} + \boldsymbol{x}^{\top} \boldsymbol{\beta} + \boldsymbol{z}^{\top} \boldsymbol{b})$

- The basis parameters are obtained by simulating from the prior distribution, this means that we simulated a random sample from a multivariate Normal distribution with mean zero and precision matrix $K(\tau) = (1/\tau) \bfs{K}_1$, where $\bfs{K}_1$ is defined in (\ref{ordem1}), and $\tau = \exp(15)$.

- We have a total of 9 scenarios with different models and sample sizes. In every scenario, we simulated $R = 500$ replicas of the model considered.

-  For the random effects model, we have considered 10, 20 and 30 observations from the same individual i, denoted by $n_{rep}$.

-  We calculated the posterior mean, and the root-mean-square error (RMSE), given by RMSE($\hat{\beta}$) = $\sqrt{\frac{1}{R} \sum_{r = 1}^R (\hat{\beta}_r - \beta)}$.
---
<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#aaa;border-spacing:0;}
.tg td{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:10px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#000;
  font-family:Arial, sans-serif;font-size:10px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-ao2g{border-color:#fff;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:fff;text-align:center;vertical-align:top}
.tg .tg-eqm3{border-color:fff;font-size:10px;text-align:left;vertical-align:top}
</style>

<font size="3">
<table class="tg">
 <caption>Posterior mean and root-mean-squared-error of the parameters for Model 1: $\Phi(\boldsymbol{a}(y)^T \boldsymbol{\gamma})$ using ILBCTM algorithm, considering 500 replicas, in the simulating study 1.</caption>
<thead>
  <tr>
    <th class="tg-ao2g" rowspan="2"><br>True parameter ($\beta$)<br></th>
    <th class="tg-baqh" colspan="3">n = 200</th>
    <th class="tg-baqh" colspan="3">n = 500</th>
    <th class="tg-baqh" colspan="3">n = 2000</th>
  </tr>
  <tr>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">3.0342</td>
    <td class="tg-0pky">3.0870</td>
    <td class="tg-0pky">0.1667</td>
    <td class="tg-0pky">3.0750</td>
    <td class="tg-0pky">0.1388</td>
    <td class="tg-0pky">3.0205</td>
    <td class="tg-0pky">0.0559</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2977</td>
    <td class="tg-0pky">-0.2883</td>
    <td class="tg-0pky">0.0522</td>
    <td class="tg-0pky">-0.2852</td>
    <td class="tg-0pky">0.0402</td>
    <td class="tg-0pky">-0.2981</td>
    <td class="tg-0pky">0.0174</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2970</td>
    <td class="tg-0pky">-0.2993</td>
    <td class="tg-0pky">0.0478</td>
    <td class="tg-0pky">-0.2977</td>
    <td class="tg-0pky">0.0313</td>
    <td class="tg-0pky">-0.2995</td>
    <td class="tg-0pky">0.0175</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.3015</td>
    <td class="tg-0pky">-0.2997</td>
    <td class="tg-0pky">0.0449</td>
    <td class="tg-0pky">-0.3015</td>
    <td class="tg-0pky">0.0322</td>
    <td class="tg-0pky">-0.3000</td>
    <td class="tg-0pky">0.0166</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2987</td>
    <td class="tg-0pky">-0.3011</td>
    <td class="tg-0pky">0.0454</td>
    <td class="tg-0pky">-0.3014</td>
    <td class="tg-0pky">0.0300</td>
    <td class="tg-0pky">-0.2993</td>
    <td class="tg-0pky">0.0140</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2948</td>
    <td class="tg-0pky">-0.3002</td>
    <td class="tg-0pky">0.0439</td>
    <td class="tg-0pky">-0.3025</td>
    <td class="tg-0pky">0.0309</td>
    <td class="tg-0pky">-0.2988</td>
    <td class="tg-0pky">0.0141</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2964</td>
    <td class="tg-0pky">-0.3020</td>
    <td class="tg-0pky">0.0468</td>
    <td class="tg-0pky">-0.3061</td>
    <td class="tg-0pky">0.0346</td>
    <td class="tg-0pky">-0.2992</td>
    <td class="tg-0pky">0.0156</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2980</td>
    <td class="tg-0pky">-0.2920</td>
    <td class="tg-0pky">0.0484</td>
    <td class="tg-0pky">-0.2995</td>
    <td class="tg-0pky">0.0332</td>
    <td class="tg-0pky">-0.2992</td>
    <td class="tg-0pky">0.0168</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2961</td>
    <td class="tg-0pky">-0.2594</td>
    <td class="tg-0pky">0.0709</td>
    <td class="tg-0pky">-0.2809</td>
    <td class="tg-0pky">0.0486</td>
    <td class="tg-0pky">-0.2978</td>
    <td class="tg-0pky">0.0180</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.2954</td>
    <td class="tg-0pky">-0.2139</td>
    <td class="tg-0pky">0.1161</td>
    <td class="tg-0pky">-0.2344</td>
    <td class="tg-0pky">0.0960</td>
    <td class="tg-0pky">-0.2961</td>
    <td class="tg-0pky">0.0196</td>
  </tr>
</tbody>
</table>
</font>


---

<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#aaa;border-spacing:0;}
.tg td{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:10px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#000;
  font-family:Arial, sans-serif;font-size:10px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-ao2g{border-color:#fff;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:fff;text-align:center;vertical-align:top}
.tg .tg-eqm3{border-color:fff;font-size:10px;text-align:left;vertical-align:top}
</style>
<font size="3">
<table class="tg">
 <caption>Posterior mean and root-mean-squared-error of the parameters for Model 2: $\Phi(\boldsymbol{a}(y)^T \boldsymbol{\gamma} + \bfs{x}^T \bfs{\beta} )$ using ILBCTM algorithm, considering 500 replicas, in the simulating study 1.</caption>
<thead>
  <tr>
    <th class="tg-ao2g" rowspan="2">True Parameter ($\beta$)</th>
    <th class="tg-baqh" colspan="2">n = 200</th>
    <th class="tg-baqh" colspan="2">n = 500</th>
    <th class="tg-baqh" colspan="2">n = 2000</th>
  </tr>
  <tr>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">-0.4792</td>
    <td class="tg-0pky">-0.4827</td>
    <td class="tg-0pky">0.0174</td>
    <td class="tg-0pky">-0.4833</td>
    <td class="tg-0pky">0.0177</td>
    <td class="tg-0pky">-0.4840</td>
    <td class="tg-0pky">0.0185</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4802</td>
    <td class="tg-0pky">-0.4829</td>
    <td class="tg-0pky">0.0162</td>
    <td class="tg-0pky">-0.4836</td>
    <td class="tg-0pky">0.0166</td>
    <td class="tg-0pky">-0.4842</td>
    <td class="tg-0pky">0.0175</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4756</td>
    <td class="tg-0pky">-0.4833</td>
    <td class="tg-0pky">0.0156</td>
    <td class="tg-0pky">-0.4841</td>
    <td class="tg-0pky">0.0164</td>
    <td class="tg-0pky">-0.4845</td>
    <td class="tg-0pky">0.0174</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4780</td>
    <td class="tg-0pky">-0.4843</td>
    <td class="tg-0pky">0.0143</td>
    <td class="tg-0pky">-0.4849</td>
    <td class="tg-0pky">0.0151</td>
    <td class="tg-0pky">-0.4849</td>
    <td class="tg-0pky">0.0155</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4767</td>
    <td class="tg-0pky">-0.4851</td>
    <td class="tg-0pky">0.0151</td>
    <td class="tg-0pky">-0.4857</td>
    <td class="tg-0pky">0.0157</td>
    <td class="tg-0pky">-0.4855</td>
    <td class="tg-0pky">0.0154</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4822</td>
    <td class="tg-0pky">-0.4856</td>
    <td class="tg-0pky">0.0144</td>
    <td class="tg-0pky">-0.4864</td>
    <td class="tg-0pky">0.0147</td>
    <td class="tg-0pky">-0.4863</td>
    <td class="tg-0pky">0.0141</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4866</td>
    <td class="tg-0pky">-0.4860</td>
    <td class="tg-0pky">0.0154</td>
    <td class="tg-0pky">-0.4869</td>
    <td class="tg-0pky">0.0154</td>
    <td class="tg-0pky">-0.4866</td>
    <td class="tg-0pky">0.0146</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4819</td>
    <td class="tg-0pky">-0.4863</td>
    <td class="tg-0pky">0.0177</td>
    <td class="tg-0pky">-0.4869</td>
    <td class="tg-0pky">0.0176</td>
    <td class="tg-0pky">-0.4868</td>
    <td class="tg-0pky">0.0164</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4834</td>
    <td class="tg-0pky">-0.4863</td>
    <td class="tg-0pky">0.0184</td>
    <td class="tg-0pky">-0.4869</td>
    <td class="tg-0pky">0.0183</td>
    <td class="tg-0pky">-0.4868</td>
    <td class="tg-0pky">0.0171</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.1500</td>
    <td class="tg-0pky">-0.1540</td>
    <td class="tg-0pky">0.0266</td>
    <td class="tg-0pky">-0.1534</td>
    <td class="tg-0pky">0.0263</td>
    <td class="tg-0pky">-0.1516</td>
    <td class="tg-0pky">0.0264</td>
  </tr>
  <tr>
    <td class="tg-0pky">-1.3000</td>
    <td class="tg-0pky">-1.3183</td>
    <td class="tg-0pky">0.0292</td>
    <td class="tg-0pky">-1.3161</td>
    <td class="tg-0pky">0.0285</td>
    <td class="tg-0pky">-1.3167</td>
    <td class="tg-0pky">0.0292</td>
  </tr>
  <tr>
    <td class="tg-0pky">0.3000</td>
    <td class="tg-0pky">0.3078</td>
    <td class="tg-0pky">0.0374</td>
    <td class="tg-0pky">0.3069</td>
    <td class="tg-0pky">0.0362</td>
    <td class="tg-0pky">0.3034</td>
    <td class="tg-0pky">0.0367</td>
  </tr>
</tbody>
</table>
</font>



---

<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#aaa;border-spacing:0;}
.tg td{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:10px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#000;
  font-family:Arial, sans-serif;font-size:10px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-ao2g{border-color:#fff;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:fff;text-align:center;vertical-align:top}
.tg .tg-eqm3{border-color:fff;font-size:10px;text-align:left;vertical-align:top}
</style>
<font size="3">
<table class="tg">
 <caption>Posterior mean and root-mean-squared-error of the parameters for Model 2: $\Phi(\boldsymbol{a}(y)^T \boldsymbol{\gamma} + \bfs{x}^T \bfs{\beta} )$ using ILBCTM algorithm, considering 500 replicas, in the simulating study 1.</caption>
<thead>
  <tr>
    <th class="tg-ao2g" rowspan="2">True Parameter ($\beta$)</th>
    <th class="tg-baqh" colspan="2">n = 200</th>
    <th class="tg-baqh" colspan="2">n = 500</th>
    <th class="tg-baqh" colspan="2">n = 2000</th>
  </tr>
  <tr>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
    <th class="tg-0pky">Estimate</th>
    <th class="tg-0pky">RMSE</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">-0.4792</td>
    <td class="tg-0pky">-0.4827</td>
    <td class="tg-0pky">0.0174</td>
    <td class="tg-0pky">-0.4833</td>
    <td class="tg-0pky">0.0177</td>
    <td class="tg-0pky">-0.4840</td>
    <td class="tg-0pky">0.0185</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4802</td>
    <td class="tg-0pky">-0.4829</td>
    <td class="tg-0pky">0.0162</td>
    <td class="tg-0pky">-0.4836</td>
    <td class="tg-0pky">0.0166</td>
    <td class="tg-0pky">-0.4842</td>
    <td class="tg-0pky">0.0175</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4756</td>
    <td class="tg-0pky">-0.4833</td>
    <td class="tg-0pky">0.0156</td>
    <td class="tg-0pky">-0.4841</td>
    <td class="tg-0pky">0.0164</td>
    <td class="tg-0pky">-0.4845</td>
    <td class="tg-0pky">0.0174</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4780</td>
    <td class="tg-0pky">-0.4843</td>
    <td class="tg-0pky">0.0143</td>
    <td class="tg-0pky">-0.4849</td>
    <td class="tg-0pky">0.0151</td>
    <td class="tg-0pky">-0.4849</td>
    <td class="tg-0pky">0.0155</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4767</td>
    <td class="tg-0pky">-0.4851</td>
    <td class="tg-0pky">0.0151</td>
    <td class="tg-0pky">-0.4857</td>
    <td class="tg-0pky">0.0157</td>
    <td class="tg-0pky">-0.4855</td>
    <td class="tg-0pky">0.0154</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4822</td>
    <td class="tg-0pky">-0.4856</td>
    <td class="tg-0pky">0.0144</td>
    <td class="tg-0pky">-0.4864</td>
    <td class="tg-0pky">0.0147</td>
    <td class="tg-0pky">-0.4863</td>
    <td class="tg-0pky">0.0141</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4866</td>
    <td class="tg-0pky">-0.4860</td>
    <td class="tg-0pky">0.0154</td>
    <td class="tg-0pky">-0.4869</td>
    <td class="tg-0pky">0.0154</td>
    <td class="tg-0pky">-0.4866</td>
    <td class="tg-0pky">0.0146</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4819</td>
    <td class="tg-0pky">-0.4863</td>
    <td class="tg-0pky">0.0177</td>
    <td class="tg-0pky">-0.4869</td>
    <td class="tg-0pky">0.0176</td>
    <td class="tg-0pky">-0.4868</td>
    <td class="tg-0pky">0.0164</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.4834</td>
    <td class="tg-0pky">-0.4863</td>
    <td class="tg-0pky">0.0184</td>
    <td class="tg-0pky">-0.4869</td>
    <td class="tg-0pky">0.0183</td>
    <td class="tg-0pky">-0.4868</td>
    <td class="tg-0pky">0.0171</td>
  </tr>
  <tr>
    <td class="tg-0pky">-0.1500</td>
    <td class="tg-0pky">-0.1540</td>
    <td class="tg-0pky">0.0266</td>
    <td class="tg-0pky">-0.1534</td>
    <td class="tg-0pky">0.0263</td>
    <td class="tg-0pky">-0.1516</td>
    <td class="tg-0pky">0.0264</td>
  </tr>
  <tr>
    <td class="tg-0pky">-1.3000</td>
    <td class="tg-0pky">-1.3183</td>
    <td class="tg-0pky">0.0292</td>
    <td class="tg-0pky">-1.3161</td>
    <td class="tg-0pky">0.0285</td>
    <td class="tg-0pky">-1.3167</td>
    <td class="tg-0pky">0.0292</td>
  </tr>
  <tr>
    <td class="tg-0pky">0.3000</td>
    <td class="tg-0pky">0.3078</td>
    <td class="tg-0pky">0.0374</td>
    <td class="tg-0pky">0.3069</td>
    <td class="tg-0pky">0.0362</td>
    <td class="tg-0pky">0.3034</td>
    <td class="tg-0pky">0.0367</td>
  </tr>
</tbody>
</table>
</font>


---
### Study 2: BCTM with Bernstein polynomials
- The main interest is to estimate the conditional distributions.

- We gonna present different scenarios which encompasses some relevant models in regression context.

- In all these scenarios we are trying to recovery the conditional density of all individuals and compare them with the real ones.

- To measure the discrepancy between the true and estimated distributions we adopt the Kullback-Leibler divergence (KLD). In all considered scenarios we use 100 repetions and compare the KLD computed from BCTM with the CTM (from the \texttt{mlt} R package), and with some well established R-packages for regression models (\texttt{gam} and \texttt{gamlss}).

- For each individual \( i = 1, \dots, n, \) we calculate the KLD between the true conditional density estimated conditional densities from different approaches

\begin{equation}
    \text{KLD}_i = \int f_{i,\text{true}}(y) \log\left(\frac{f_{i,\text{true}}(y)}{f_{i,\text{est}}(y)}\right) \, dy,
\end{equation}

The calculated KLD is given by

\begin{equation}\label{kld_approx}
    \text{KLD}_i = \sum_{s = 1}^S f_{i,\text{true}}(y_s) \log\left(\frac{f_{i,\text{true}}(y_s)}{f_{i,\text{est}}(y_s)}\right) \, \Delta y_s,
\end{equation}

To summarize, we computed the mean KLD, defined as $\overline{KLD} = \sum_{i = 1}^n \frac{KLD_i}{n}$.


Our study includes 48 scenarios, combining variations in parameters and sample sizes. For each scenario, we generated \( R = 100 \) replicates of samples of size $n$ from the model \ref{model_simu_original}. For each replicate $r =1, \hdots, 100$,  we computed $\overline{KLD}_t$

---
#### Regression models with fixed effects

$$\begin{align}\label{model_simu_original}
Y_{i}| \boldsymbol{x}_i &\sim SN(\mu_{i}, \sigma, \lambda) \\
\mu_{i} &= \boldsymbol{x}_{i}^{\top}\boldsymbol{\beta}\nonumber
\end{align}$$

\begin{equation}
    F_{Y_i|\bfs{X}_i = \bfs{x}_i}(Y_i \leq y_i | \bfs{X}_i = \bfs{x}_i) = \Phi(\boldsymbol{a}(y_i)^{\top}\boldsymbol{\gamma} + \boldsymbol{x}_i^{\top}\boldsymbol{\beta})
\end{equation}


for $i = 1, \hdots, n$, where $\boldsymbol{a}(y)^T\boldsymbol{\gamma}$ is the monotonic Bernstein construction from Section \ref{Bernstein_section} of degree $k$, and $\boldsymbol{\gamma} $ are the Bernstein coefficients.  The model can be summarized as 


$$\begin{align}\label{model1_simu}
    h(Y|\boldsymbol{x}, \boldsymbol{\beta}) & = \boldsymbol{a}(Y)^\top \boldsymbol{\gamma} + \boldsymbol{x}^\top \boldsymbol{\beta}, \nonumber \\
    Z & \overset{d}{=} h(Y|\boldsymbol{x}, \boldsymbol{\beta}) \sim N(0,1), \nonumber \\
    Y|\boldsymbol{x} & \overset{d}{=} h^{-1}(Z), \nonumber \\
    \log(\boldsymbol{\gamma}) & \sim N\left(0, \frac{1}{\tau_1} \bfs{K_1}\right), \nonumber \\
    \boldsymbol{\beta} & \sim N_4(0, 10^{-6} \times I_4), \nonumber \\
    \tau_1 & =  \exp(\theta_1),\\
     \exp(\theta_1) & \sim \text{Inv-Gamma}(1,0.001), \nonumber
\end{align}$$

- We benchmark the model in (\ref{model1_simu}) fitted by the algorithm proposed in Section \ref{VBC_BCTM}, against a CTM model fitted with the mltR package.
- Additionally, our comparison include the orginal model from (\ref{model_simu_original}), fitted with the gamlss package. It serves as a gold standard because it aligns with the true model used to simulate the data.

---
### Applications
___

- For comparison purposes we decided to implement our algorithm in models which already been applied in three different data sets.
- The first application is a density estimation presented by `r Citet(myBib,"hothorn2018most")` using CTM with the *mlt* R package.
- The second is a regression problem with only one explanatory variable also applied using the *mlt* R package.
- The third one is presented by `r Citet(myBib,"carlan2023bayesian")` and involves more explanatory variables and random effects in a BCTM. The application was developed using MCMC algorithm


---

<table border="1" cellpadding="5" cellspacing="0" style="border-collapse:collapse; font-size: 12px;">
  <caption>Estimated average KLD for BCTM, CTM and Skew Normal models across different sample sizes, basis dimension and parameter settings from model with p = 4, considering 100 replicas in simulation study 2 for the fixed regression models</caption>
  <thead>
    <tr>
      <th colspan="7" style="text-align:center;">&#963; = 1, &#955; = -2</th>
    </tr>
    <tr>
      <th rowspan="3">Model</th>
      <th colspan="3" style="text-align:center;">k = 10</th>
      <th colspan="3" style="text-align:center;">k = 20</th>
    </tr>
    <tr>
      <th colspan="3" style="text-align:center;">n</th>
      <th colspan="3" style="text-align:center;">n</th>
    </tr>
    <tr>
      <th>500</th>
      <th>1000</th>
      <th>2000</th>
      <th>500</th>
      <th>1000</th>
      <th>2000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BCTM</td>
      <td>0.00531</td>
      <td>0.00297</td>
      <td>0.00161</td>
      <td>0.00517</td>
      <td>0.00278</td>
      <td>0.00149</td>
    </tr>
    <tr>
      <td>CTM</td>
      <td>0.00894</td>
      <td>0.00492</td>
      <td>0.00257</td>
      <td>0.00954</td>
      <td>0.00533</td>
      <td>0.00283</td>
    </tr>
    <tr>
      <td>Skew Normal</td>
      <td>0.00510</td>
      <td>0.00277</td>
      <td>0.00148</td>
      <td>0.00481</td>
      <td>0.00256</td>
      <td>0.00132</td>
    </tr>
    <tr>
      <td colspan="7" style="text-align:center;">&#963; = 1, &#955; = 2</td>
    </tr>
    <tr>
      <td>BCTM</td>
      <td>0.00511</td>
      <td>0.00287</td>
      <td>0.00159</td>
      <td>0.00485</td>
      <td>0.00275</td>
      <td>0.00148</td>
    </tr>
    <tr>
      <td>CTM</td>
      <td>0.00858</td>
      <td>0.00491</td>
      <td>0.00256</td>
      <td>0.00901</td>
      <td>0.00535</td>
      <td>0.00282</td>
    </tr>
    <tr>
      <td>Skew Normal</td>
      <td>0.00479</td>
      <td>0.00277</td>
      <td>0.00148</td>
      <td>0.00461</td>
      <td>0.00255</td>
      <td>0.00132</td>
    </tr>
  </tbody>
</table>

---
### Regression models with non-linear effects

- Case (a): $\mu_{i} = \boldsymbol{x}_{i}^{\top}\boldsymbol{\beta} + f_1(z_{i1})$; 
- Case (b): $\mu_{i} = \boldsymbol{x}_{i}^{\top}\boldsymbol{\beta} + f_1(z_{i1}) + f_2(z_{i2})$;
- Case (c): $\mu_{i} = \boldsymbol{x}_{i}^{\top}\boldsymbol{\beta} + f(z_{i3},z_{i4})$.

where  $f_1(z_1) = sin(5\pi z_1), f_2(z_2) = z_2 + z_2^2,$ and $f(z_3,z_4) = (z_3 + 2z_3^2)exp(z_4/2)$

- Samples sizes $n = 500,1000,2000$; two distinct $\boldsymbol{\beta}$ vectors of length $p =2$ and 4, $\boldsymbol{\beta} = (\beta_0, \beta_1) = (0.5,2)$, $\boldsymbol{\beta} = (\beta_0, \beta_1, \beta_2,\beta_3, \beta_4) = (0.5,-1.5,-0.2,1)$. Moreover, $\sigma = 1$, $\lambda= -2$ and 2. 

- For the case (c) we assumed the same scenarios, but only $p = 4$.

- The covariates $x_{i1}$ and $x_{i3}$ are sampled from standard Normal distributions, and $x_{i2}$ from a binomial distributions with probabilities 0.3. 

- The covariates $z_{i1}$ and $z_{i2}$ are sampled from an uniform distribution in (0,1), and $z_{i3}$, $z_{i4}$ are sampled from standard Normal distributions.
---


$$\begin{align}
    h(Y|\boldsymbol{x}, z, \boldsymbol{\beta}) & = \boldsymbol{a}(Y)^\top \boldsymbol{\gamma} + \boldsymbol{b}(z)^{\top}\boldsymbol{\beta}_1 + \boldsymbol{x}^\top \boldsymbol{\beta}_2, \nonumber \\
    Z & \overset{d}{=} h(Y|\boldsymbol{x}, \boldsymbol{\beta}) \sim N(0,1), \nonumber \\
    Y|\boldsymbol{x} & \overset{d}{=} h^{-1}(Z), \nonumber \\
    \log(\boldsymbol{\gamma}) & \sim N\left(0, \frac{1}{\tau_1} \bfs{K_1}\right), \nonumber \\
    \boldsymbol{\beta}_1 & \sim N\left(0, \frac{1}{\tau_2} \bfs{K_2}\right), \nonumber\\
    \boldsymbol{\beta}_2 & \sim N(0, 10^{-6} \times I_4), \nonumber \\
    \tau_l & =  \exp(\theta_l), l = 1, 2, \nonumber\\
     \exp(\theta_l) & \sim \text{Inv-Gamma}(1,0.001), l = 1, 2, \nonumber
\end{align}$$

- We benchmark this model against the one defined in \ref{model_simu_original}, with $\mu$ specified as in case (a),  fitted by the \texttt{gamlss} R package. 

- The \texttt{gamlss} package enables the fitting of additive models that account for non-linear effects of explanatory variables through cubic splines, implemented with the \texttt{s()} function with $k$ knots. 

- This model is not currently supported by the \texttt{mlt} package.

---
<html>
<head>
    <title>Estimated Average KLD Table</title>
    <style>
        table {
            border-collapse: collapse;
            width: 100%;
            font-size: 0.9em;
        }
        th, td {
            border-top: 1px solid #000;
            text-align: center;
            padding: 8px;
        }
        th {
            background-color: #ffffff;
        }
    </style>
</head>
<body>
    <h2>Estimated Average KLD Table</h2>
    <p>Estimated average &macr;KLD for BCTM and Skew Normal models across different sample sizes, basis dimension, and parameter settings, case (a), with p = 4, considering 100 replicas in simulation study for the non-linear regression model.</p>

    <table>
        <tr>
            <th></th>
            <th colspan="6">σ = 1, λ = 2</th>
        </tr>
        <tr>
            <th>Model</th>
            <th colspan="3">k = 10</th>
            <th colspan="3">k = 15</th>
        </tr>
        <tr>
            <th></th>
            <th>500</th><th>1000</th><th>2000</th>
            <th>500</th><th>1000</th><th>2000</th>
        </tr>
        <tr>
            <td>BCTM</td>
            <td>0.03818</td><td>0.02705</td><td>0.02108</td>
            <td>0.03984</td><td>0.02839</td><td>0.02245</td>
        </tr>
            <td>Skew Normal</td>
            <td>0.03452</td><td>0.02665</td><td>0.02293</td>
            <td>0.03919</td><td>0.02874</td><td>0.02382</td>
        </tr>
        <tr>
            <th></th>
            <th colspan="6">σ = 1, λ = -2</th>
        </tr>
        <tr>
            <th>Model</th>
            <th colspan="3">k = 10</th>
            <th colspan="3">k = 15</th>
        </tr>
        <tr>
            <th></th>
            <th>500</th><th>1000</th><th>2000</th>
            <th>500</th><th>1000</th><th>2000</th>
        </tr>
        <tr>
            <td>BCTM</td>
            <td>0.03781</td><td>0.02693</td><td>0.02145</td>
            <td>0.04001</td><td>0.02882</td><td>0.02266</td>
        </tr>
            <td>Skew Normal</td>
            <td>0.03459</td><td>0.02660</td><td>0.02292</td>
            <td>0.03930</td><td>0.02867</td><td>0.02377</td>
        </tr>
        <tr>
    </table>
</body>
</html>



---

$$\begin{align}
    h(Y|\boldsymbol{x}, z, \boldsymbol{\beta}) & = \boldsymbol{a}(Y)^\top \boldsymbol{\gamma} + \boldsymbol{b}(z_1)^{\top}\boldsymbol{\beta}_1 + \boldsymbol{b}(z_2)^{\top}\boldsymbol{\beta}_2 + \boldsymbol{x}^\top \boldsymbol{\beta}_3, \nonumber \\
    Z & \overset{d}{=} h(Y|\boldsymbol{x}, \boldsymbol{\beta}) \sim N(0,1), \nonumber \\
    Y|\boldsymbol{x} & \overset{d}{=} h^{-1}(Z), \nonumber \\
    \log(\boldsymbol{\gamma}) & \sim N\left(0, \frac{1}{\tau_1} K_1\right), \nonumber \\
    \boldsymbol{\beta}_1 & \sim N\left(0, \frac{1}{\tau_2} K_2\right), \nonumber\\
    \boldsymbol{\beta}_2 & \sim N\left(0, \frac{1}{\tau_3} K_2\right), \nonumber\\
    \boldsymbol{\beta}_3 & \sim N(0, 10^{-6} \times I_4), \nonumber \\
    \tau_l & =  \exp(\theta_l), l = 1, 2,3, \nonumber\\
     \exp(\theta_l) & \sim \text{Inv-Gamma}(1,0.001), l = 1, 2,3. \nonumber
\end{align}$$

- We benchmark this model against the one defined in \ref{model_simu_original}, with $\mu$ specified as in case (b),  fitted by the \texttt{gamlss} R package. 

- This model is not currently supported by the \texttt{mlt} package.

---
<html>
<head>
    <title>Estimated Average KLD Table</title>
    <style>
        table {
            border-collapse: collapse;
            width: 100%;
            font-size: 0.9em;
        }
        th, td {
            border-top: 1px solid #000;
            text-align: center;
            padding: 8px;
        }
        th {
            background-color: #ffffff;
        }
    </style>
</head>
<body>
    <h2>Estimated Average KLD Table</h2>
    <p>Estimated average &macr;KLD for BCTM and Skew Normal models across different sample sizes, basis dimension, and parameter settings, case (b), with p = 2, considering 100 replicas in simulation study for the non-linear regression model.</p>

    <table>
        <tr>
            <th></th>
            <th colspan="6">σ = 1, λ = 2</th>
        </tr>
        <tr>
            <th>Model</th>
            <th colspan="3">k = 10</th>
            <th colspan="3">k = 15</th>
        </tr>
        <tr>
            <th></th>
            <th>500</th><th>1000</th><th>2000</th>
            <th>500</th><th>1000</th><th>2000</th>
        </tr>
        <tr>
            <td>BCTM</td>
            <td>0.05227</td><td>0.03102</td><td>0.02445</td>
            <td>0.08246</td><td>0.03744</td><td>0.02869</td>
        </tr>
        <tr>
            <td>Skew Normal</td>
            <td>0.04298</td><td>0.03016</td><td>0.02507</td>
            <td>0.05101</td><td>0.03408</td><td>0.02661</td>
        </tr>
        <tr>
            <th></th>
            <th colspan="6">σ = 1, λ = -2</th>
        </tr>
        <tr>
            <th>Model</th>
            <th colspan="3">k = 10</th>
            <th colspan="3">k = 15</th>
        </tr>
        <tr>
            <th></th>
            <th>500</th><th>1000</th><th>2000</th>
            <th>500</th><th>1000</th><th>2000</th>
        </tr>
        <tr>
            <td>BCTM</td>
            <td>0.05271</td><td>0.03103</td><td>0.02456</td>
            <td>0.08593</td><td>0.03745</td><td>0.02898</td>
        </tr>
        <tr>
            <td>Skew Normal</td>
            <td>0.04316</td><td>0.02997</td><td>0.02501</td>
            <td>0.05118</td><td>0.03393</td><td>0.02657</td>
        </tr>
    </table>
</body>
</html>
---

### Application 1 
___

- The data set of this application contains the waiting time between eruptions from the Old Faithful geyser in Yellowstone National Park.
- The response variable,  $y$, is the waiting time, and no explanatory variables are incorporated into the model.
- The goal is to estimate the density distribution of $y$ using the following BCTM

$$\begin{equation}\label{geys_F}
    F_Y(y) = F_Z(h(y)) = F_Z(\boldsymbol{a}(y)^T \boldsymbol{\gamma}),
\end{equation}$$
where $\boldsymbol{a}$ is a vector of evaluated B-spline basis of dimension eight. The vector $\boldsymbol{\gamma} = (\gamma_1, \dots, \gamma_8)$ is the basis coefficients model.
- The *mlt* package implements the CTM model using the Bernstein basis of dimension eight instead of the B-spline basis.




---
.pull-left[

```{r, echo=FALSE, out.width = "90%", dpi = 300 , fig.cap="Estimated density for waiting times between eruptions by the mlt package (dashed line) and BCTM model (continuous line)", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/dens_geys2.png")
```

]

.pull-right[
```{r, echo=FALSE, out.width = "90%", dpi = 300 , fig.cap="Estimated density with the respective credible interval (dashed line) for waiting times between eruptions for BCTM model", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/dens_gey_HPD2.png")
```
]


<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#aaa;border-spacing:0;}
.tg td{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:10px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#fff;border-color:#aaa;border-style:solid;border-width:0px;color:#000;
  font-family:Arial, sans-serif;font-size:10px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-ao2g{border-color:#fff;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:fff;text-align:center;vertical-align:top}
.tg .tg-eqm3{border-color:fff;font-size:10px;text-align:left;vertical-align:top}
</style>
<font size="3">
<table class="tg">
 <caption>Quantiles of estimated distribution.</caption>
<thead>
  <tr>
    <th class="tg-ao2g" rowspan="2"><br>Model<br></th>
    <th class="tg-baqh" colspan="5">Quantile</th>
  </tr>
  <tr>
    <th class = "tg-0pky">0.05</th>
    <th class = "tg-0pky">0.25</th>
    <th class = "tg-0pky">0.5</th>
    <th class = "tg-0pky">0.75</th>
    <th class = "tg-0pky">0.95</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class = "tg-0pky">BCTM</td>
    <td class = "tg-0pky">49.20</td>
    <td class = "tg-0pky">58.90</td>
    <td class = "tg-0pky">76.20</td>
    <td class = "tg-0pky">83.10</td>
    <td class = "tg-0pky">91.00</td>
  </tr>
  <tr>
    <td class = "tg-0pky">mlt</td>
    <td class = "tg-0pky">49.15</td>
    <td class = "tg-0pky">59.21</td>
    <td class = "tg-0pky">75.81</td>
    <td class = "tg-0pky">83.27</td>
    <td class = "tg-0pky">90.77</td>
  </tr>
</tbody>
</table>

<!---
![:scale 80%](figuras/dens_gey_HPD.png)

.caption[
Image caption
]

]

.pull-right[
![:scale 80%](figuras/dens_gey_HPD.png)

.caption[
Image caption
]
]
--->
---
### Application 2  
___
- We considered a data set which the observations are measures of age ( $x$ ) and head circumference ( $y$ ) of 7040 young Dutch boys.

```{r,  results='asis', echo=FALSE, message=FALSE, warning = FALSE}
library(gamlss)
data("db",package = "gamlss.data")
attach(db)
library(ggplot2)
library(ggExtra)
```

```{r,results='asis', echo=FALSE, message=FALSE, warning = FALSE,  fig.retina=2, fig.align='center',out.width = "40%"}
# Save the scatter plot in a variable
p <- ggplot(db, aes(x = age, y = head)) +
  geom_point(col = "grey", size = 0.51) + theme_classic() +
  xlab("Age") + ylab("Head Circumference")+theme(axis.text=element_text(size=20),
                                   axis.title=element_text(size=14,face="bold"))

# Plot the scatter plot with marginal histograms
ggMarginal(p, type = "histogram",size = 2, fill = "lightblue")

```

---

- `r Citet(myBib,"hothorn2018most")` used the *mlt* package to fit a conditional transformation model for head circumference with explanatory variable $x = age^{1/3}$. 
- We first propose a BCLTM and a linear regression to observe how a restricted model fit this data.

.pull-left[

- BCLTM
$${\small F_Y(y|\boldsymbol{x}) = F_Z(\beta_0 + y\cdot\beta_1 + y\cdot age \cdot \beta_2 + \beta_3 \cdot x)}$$
 
```{r, echo=FALSE,  out.width = "100%", dpi = 300 , fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Linear_bctm.png")
```


]

.pull-right[

- Linear regression
$${\small y_i \sim N(\beta_0 + \beta_1 x_i, \sigma)}$$

```{r, echo=FALSE,  out.width = "100%", dpi = 300 ,  fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Lm_IC.png")
```
] 


---
- We also fitted a BCTM model which considers a non-linear interaction between $y$ and $x$. The model is given by

$$\begin{equation}
    F_Y(y|\boldsymbol{x}) = F_Z(h(y|\boldsymbol{x})) = F_Z(\boldsymbol{a}(y)\otimes \boldsymbol{b}(age) \boldsymbol{\gamma})^T 
\end{equation}$$

where $\boldsymbol{a}$ and $\boldsymbol{b}(age)$ are vectors of evaluated cubic B-spline basis of dimension 7.

.pull-left[
- BCTM

```{r, echo=FALSE,  out.width = "95%",  dpi = 300 , fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/y_age_13_cp.png")
```
 
]
.pull-right[
- CTM

```{r, echo=FALSE,  out.width = "95%",  dpi = 300, fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/y_age_ctm_13_cp.png")
```
] 

---

- In the first model only the expected value and variance are affected by age.

.pull-left[
```{r, echo=FALSE, out.width = "110%", dpi = 300 , fig.cap="Expected value (E(Y)), standard deviation (SD(Y)), Skewness and Kurtosis of BCLTM conditional on Age", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Mom_linear.png")
```
 
]
.pull-right[
```{r, echo=FALSE, out.width = "110%", dpi = 300 , fig.cap="Expected value (E(Y)), standard deviation (SD(Y)), Skewness and Kurtosis of BCTM conditional on x", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Mom_bctm_13.png")
```
] 

---
<!---class: columns-2
--->

### Application 3
___

- The dataset for the third application comes from the Framingham Heart Study. In 1948, the Framingham Heart Study started a project to identify common factors that contribute to cardiovascular disease by following group of participants over a long period.
- `r Citet(myBib,"carlan2023bayesian")` randomly selected 200 participants from the study and their respective cholesterol levels, gender and age measured at the beginning of the study and every two years for ten years.

<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-color:#ccc;border-spacing:0;}
.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:0px;color:#333;
  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<font size="3">
<table class="tg">
 <caption>First six observations from the Framingham dataset</caption>
    <tr>
        <th>Id</th>
        <th>Cholesterol level</th>
        <th>Sex</th>
        <th>Age</th>
        <th>Year</th>
    </tr>
    <tr>
        <td>1</td>
        <td>175</td>
        <td>1</td>
        <td>32</td>
        <td>0</td>
    </tr>
    <tr>
        <td>1</td>
        <td>198</td>
        <td>1</td>
        <td>32</td>
        <td>2</td>
    </tr>
    <tr>
        <td>1</td>
        <td>205</td>
        <td>1</td>
        <td>32</td>
        <td>4</td>
    </tr>
    <tr>
        <td>1</td>
        <td>228</td>
        <td>1</td>
        <td>32</td>
        <td>6</td>
    </tr>
    <tr>
        <td>1</td>
        <td>214</td>
        <td>1</td>
        <td>32</td>
        <td>8</td>
    </tr>
    <tr>
        <td>1</td>
        <td>214</td>
        <td>1</td>
        <td>32</td>
        <td>10</td>
    </tr>
</table>

---
- Carlan proposed two different models to predict and make inferences about cholesterol individuals based on their sex and age.
- The models proposed estimated a different probability distribution for the individual's cholesterol for each age and sex.
- The first one is given by 

$$\begin{align}
P(cholst \leq y | \boldsymbol{x}) & = \Phi(h1(y) + h_2(y|age) + h_3(y|year) + h_4(y|sex))  \nonumber \\
& =  \Phi(\gamma_0 + \boldsymbol{a}(y)^{T}\boldsymbol{\gamma_1} + \textrm{age} \cdot \boldsymbol{a}(y)^{T}\boldsymbol{\gamma_2}  + \textrm{year} \cdot \gamma_3 + \textrm{sex} \cdot \gamma_4) \end{align}$$
- The B-spline employed in $\boldsymbol{a}$ is bases of dimension $D_1 = 20$, and it implies that the vector $\boldsymbol{\gamma} = (\gamma_0, \boldsymbol{\gamma}_1, \boldsymbol{\gamma}_2, \boldsymbol{\gamma}_3, \boldsymbol{\gamma}_4)$ has 42 parameters.
- The prior precision matrix for the splines parameters $\boldsymbol{\gamma}_1$ and $\boldsymbol{\gamma}_2$ considers one hyper-parameter

$$\begin{equation}\label{vcm_matrix}
\boldsymbol{K} = \frac{1}{\tau}\begin{pmatrix}
 \boldsymbol{K_1} & 0\\
 0 & \boldsymbol{K_2}
\end{pmatrix}.
\end{equation}$$
- The $\boldsymbol{K}_1$ and $\boldsymbol{K}_1$ are the first-order penalization matrices.
---

- In the second model we assume a non-linear interaction between cholesterol level and age

$$\begin{align}
P(cholst \leq y | \boldsymbol{x}) &  = \Phi(h1(y|age) + h_2(y|year) + h_3(y|sex)) \nonumber \\
& = \Phi((\boldsymbol{a}(y)^{T} \otimes \boldsymbol{b}_1(\textrm{age})^T)^T \boldsymbol{\gamma_1} + \textrm{year} \cdot \gamma_2 + \textrm{sex} \cdot \gamma_3)
\end{align}$$
which both $\boldsymbol{a}(y)$ and $\boldsymbol{b}(x)$ consist of 10-dimensional B-splines basis.
- The vector of parameters $\boldsymbol{\gamma} = (\gamma_0,\boldsymbol{\gamma}_1, \gamma_2, \gamma_3)$ has 102 parameters.
- We considered the following precision matrix

$$\begin{equation}\label{tensor_matrix}
    K(\tau) = \frac{1}{\tau}[(\boldsymbol{K}_1 \otimes \boldsymbol{I}_{10}) + (\boldsymbol{I}_{10} \otimes \boldsymbol{K}_2 )],
\end{equation}$$
- Additionally, both models augmented with individual-specific i.i.d random effects were considered.
- The precision matrix for normal multivariate with random effects is constructed as

$$\boldsymbol{K}_{re} = \begin{pmatrix}
 \boldsymbol{K}(\tau) & 0\\
 0 &  \tau_{re} \boldsymbol{I}
\end{pmatrix}.$$

---
- We benchmark our implemented algorithm against the NUTS algorithm. 
- The size chain for the NUTS algorithm was 4000 with 2000 as burn-in.


<style type="text/css">
.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}
.tg td{background-color:#fff;border-bottom-width:1px;border-color:#ccc;border-style:solid;border-top-width:1px;
  border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:16px;overflow:hidden;padding:10px 5px;
  word-break:normal;}
.tg th{background-color:#f0f0f0;border-bottom-width:1px;border-color:#ccc;border-style:solid;border-top-width:1px;
  border-width:0px;color:#333;font-family:Arial, sans-serif;font-size:16px;font-weight:normal;overflow:hidden;
  padding:10px 5px;word-break:normal;}
.tg .tg-xvg5{background-color:#f9f9f9;border-color:#aaaaaa;text-align:left;vertical-align:top}
.tg .tg-buh4{background-color:#f9f9f9;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-dzk6{background-color:#f9f9f9;text-align:center;vertical-align:top}
.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}
</style>
<font size="4">
<table class="tg">
 <caption>DIC and elapsed time second from the two models fitted by different algorithms.</caption>
<thead>
  <tr>
    <th class="tg-0lax">Model</th>
    <th class="tg-0lax">Method</th>
    <th class="tg-0lax">DIC</th>
    <th class="tg-0lax">Time</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-dzk6" rowspan="2"><br>Model 1</td>
    <td class="tg-buh4">INLA based</td>
    <td class="tg-buh4">2769.26</td>
    <td class="tg-buh4">21.98</td>
  </tr>
  <tr>
    <td class="tg-73oq">NUTS</td>
    <td class="tg-73oq">2787.02</td>
    <td class="tg-73oq">1027.12</td>
  </tr>
  <tr>
    <td class="tg-dzk6" rowspan="2">Model 1<br><br>random effects</td>
    <td class="tg-xvg5">INLA based</td>
    <td class="tg-xvg5">1604.01</td>
    <td class="tg-xvg5">619.05</td>
  </tr>
  <tr>
    <td class="tg-0lax">NUTS</td>
    <td class="tg-0lax">1567.38</td>
    <td class="tg-0lax">10716.02</td>
  </tr>
  <tr>
    <td class="tg-dzk6" rowspan="2"><br>Model 2</td>
    <td class="tg-buh4">INLA based</td>
    <td class="tg-buh4">2721.04</td>
    <td class="tg-buh4">38.88</td>
  </tr>
  <tr>
    <td class="tg-0lax">NUTS</td>
    <td class="tg-0lax">2716.30</td>
    <td class="tg-0lax">2761.90</td>
  </tr>
  <tr>
    <td class="tg-dzk6" rowspan="2">Model 2<br>random effects</td>
    <td class="tg-buh4">INLA based</td>
    <td class="tg-buh4">1562.99</td>
    <td class="tg-buh4">1551.29</td>
  </tr>
  <tr>
    <td class="tg-0lax">NUTS</td>
    <td class="tg-0lax">1568.88</td>
    <td class="tg-0lax">22735.07</td>
  </tr>
</tbody>
</table>

---


.pull-left[
```{r, echo=FALSE, out.width = "60%", dpi = 300 , fig.cap="Coefficients from the VCM model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/coefs_vcm2.png")
```

```{r, echo=FALSE, out.width = "55%", dpi = 300 , fig.cap="Coefficients from the VCM random effects model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/coefs_vcm_re2.png")
```
 
]
.pull-right[
```{r, echo=FALSE, out.width = "55%", dpi = 300 , fig.cap="Coefficients from the Tensor model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/coef_tensor2.png")
```

```{r, echo=FALSE, out.width = "55%", dpi = 300 , fig.cap="Coefficients from the Tensor random effects model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/coef_tensor_re2.png")
```
] 


---

.pull-left[
```{r, echo=FALSE, out.width = "110%", dpi = 300 , fig.cap="95% quantile interval and the median for every individual posterior cumulative distribution estimated by INLA based algorithm for the VCM random effects model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Tensor_vcm_IC.png")
```

]

.pull-right[
```{r, echo=FALSE, out.width = "110%", dpi = 300 , fig.cap="95% quantile interval and the median for every individual posterior cumulative distribution estimated by stochastic simulation for the VCM random effects model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Tensor_vcm_IC_simu.png")
```
]
---
 

.pull-left[
```{r, echo=FALSE, out.width = "110%", dpi = 400 , fig.cap="95% quantile interval and the median for every individual posterior cumulative distribution estimated by INLA based algorithm for the tensor random effects model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Tensor_re_IC.png")
```
]

.pull-right[
```{r, echo=FALSE, out.width = "110%", dpi = 400 , fig.cap="95% quantile interval and the median for every individual posterior cumulative distribution estimated by stochastic simulation for the tensor random effects model.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/Tensor_re_IC_sim.png")
```
] 

---

- Let us consider the Tensor model again. We can see the impact of age on the cholesterol level of male individuals in the middle of study.


.pull-left[
```{r, echo=FALSE, out.width = "90%", dpi = 300 , fig.cap="Posterior estimated densities of the Tensor model for different male individual ages in the middle of the study. Shown are the estimated.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/chol_dist_age2.png")
```
]

.pull-right[
```{r, echo=FALSE, out.width = "90%", dpi = 300 , fig.cap="Expected value (E(Y)), standard deviation (SD(Y)), Skewness and Kurtosis of Tensor model conditional on Age.", fig.retina = 2,fig.align='center'}
knitr::include_graphics("figuras/moments2.png")
```
]
---
### Final remarks
___
- The BCTM targets a flexible estimation leaving aside any parametric and linear restriction on conditional transformation function. However, flexibility comes with a price, and the interpretability of the model is restrictive. 
- Taking apart some discrepancies between the fitted models, our alternative estimation process obtained similar DIC and estimated quantile posterior intervals for the observations. 
- The main result of this application is the gap between the elapsed time provided by the two Bayesian estimation procedures.
- A simulation study for the recovery of parameters model and transformation function will guide us to detect any algorithm problem. 
- Some alterations in the BCTM models presented here are available for discussion.
  - A multivariate CTM, where $F_Z$ is a multivariate Gaussian distribution.
  - Bernstein polynomial could be a alternative for the B-spline.
  - A Deep CTM using neural network predictors.
---
- Some aspects should be improved in the algorithm.
  - Can we reduce the points of exploration from hyperparameter distribution?
  - A faster algorithm to find the mode of distribution of the hyperparameters.
  - Extend the algorithm for more than two hyperparameters.
- Residual analysis.

---

class: center, middle

Obrigado!





